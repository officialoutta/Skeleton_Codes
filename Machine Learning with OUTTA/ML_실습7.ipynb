{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ccf892",
   "metadata": {},
   "source": [
    "# 실습 7. MyNN 라이브러리를 사용하여 실습 6 다시 해결하기  \n",
    "  \n",
    "실습 6에서는 신경망의 구조를 직접 설계하고, 경사하강법을 코딩을 통해 직접 구현하여 점을 그 점이 속하는 소용돌이로 분류하는 머신러닝 문제를 해결해보았다. 이번 실습의 궁극적인 목표는 실습 6과 동일하게, 점을 분류하는 문제를 해결하는 것이다. 하지만 이번에는 머신러닝을 구현하는 데 있어서 특화되어있는 라이브러리, MyNN의 사용법을 배우고 이를 활용하여 문제를 더 간편하게 해결하보자. 이에 본 실습에서는 실습 6에서 다루었던 내용을 거의 생략하였으니, 관련 내용이 기억이 나지 않는다면 적절히 복습하도록 하자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3faf54",
   "metadata": {},
   "source": [
    "## Step 1. 필요한 라이브러리 import 하기, 데이터 가져오기  \n",
    "  \n",
    "이번 실습을 진행하는 데 필요한 라이브러리를 import 하고, 데이터를 가져오는 과정이다. 실습 6의 Step 1과 완전히 동일한 과정을 진행하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25085b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5045efbc",
   "metadata": {},
   "source": [
    "## Step 2. 모델의 학습 정확도를 계산하는 함수 작성하기  \n",
    "  \n",
    "실습 6의 Step 6과 동일한 함수 accuracy를 작성하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb6411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e029d4e",
   "metadata": {},
   "source": [
    "## Step 3. MyNN 라이브러리로 밀집 신경망 구축하는 방법 익히기  \n",
    "  \n",
    "이번 실습에서, 그리고 앞으로의 실습들에서 활용하게 될 라이브러리 MyNN은 신경망을 구축하고 관리하거나 학습시키는 데 있어 필요로 하는 기본적인 기능들을 미리 만들어놓은 라이브러리이다. 이번 Step 3에서는 밀집 신경망(dense neural network)를 구축하고, 이를 이용하여 각 노드에서의 출력을 계산해보자. 이후의 Step 6에서는 라이브러리에 내장되어있는 optimizer를 활용해보는 기능도 사용해볼 것이다. 추가적으로, 이번 실습에서 소개하지 않은 MyNN 라이브러리의 다른 기능이 궁금하다면, 라이브러리에 포함되는 클래스 등의 코드가 들어있는 이 링크(https://github.com/davidmascharka/MyNN) 를 확인해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2890cd4",
   "metadata": {},
   "source": [
    "우선, 아래 코드를 사용하여 MyNN 라이브러리에 구축되어있는 클래스인 dense를 import 해오자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3466f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.layers.dense import dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c08bf",
   "metadata": {},
   "source": [
    "dense 클래스는 신경망에서의 하나의 밀집층을 구현해놓은 것이다. 객체를 생성할 때는, 입력 데이터와 출력 데이터의 차원을 입력하면 자동으로 가중치(weight)와 편향(bias)를 초기화해준다. 다양한 초기화 방법을 설정할 수 있으니, 필요한 방법이 있으면 라이브러리 링크를 참고하여 적절히 사용해보자. 객체를 생성한 이후, 입력해준 입력 데이터의 차원을 갖는 데이터를 이 층에 통과시켜주면 가중치와 편향을 반영한 출력값을 내어준다. 따라서 실제로 모델을 학습시킬 때, 활성함수 등은 따로 지정하여 구현해주어야 한다. 참고로, Keras라는 MyNN과 비슷한 기능의 라이브러리에서는 활성함수도 매개변수로 지정하여 사용할 수 있게 해 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087d11f5",
   "metadata": {},
   "source": [
    "우선, 클래스의 객체를 하나 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터의 차원 2, 출력 데이터의 차원 3인 밀집 신경망을 초기화하는 코드\n",
    "# 해당 밀집 신경망의 구축에 필요한 가중치인 shape-(2, 3) mygrad 텐서와 편향 shape-(3,) mygrad 텐서가 자동으로 초기화됨\n",
    "# 기본적인 가중치 초기화 방법은 균등분포로부터의 랜덤 추출\n",
    "# 기본적인 편향의 초기화 방법은 상수(0)로 초기화\n",
    "\n",
    "dense_layer = dense(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254586b7",
   "metadata": {},
   "source": [
    "교재의 그림 2.4-31은 위 코드를 실행했을 때 초기화되는 신경망의 모식도이므로, 참고하자.  \n",
    "이제 가중치와 편향을 확인해보고, 임의의 데이터에 대한 출력값 계산을 각각 코드로 어떻게 구현하는지 확인해보자. 모식도를 참고하여 아래 코드들의 실행 결과를 예상해본 이후, 직접 실행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9544567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 객체 dense_layer의 weight를 출력\n",
    "# 의도한 shape-(2, 3) 형태의 tensor가 생성되었는지 확인해보자\n",
    "dense_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df95c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 객체 dense_layer의 bias를 출력\n",
    "# 의도한 shape-(3,) 형태의 tensor가 생성되었는지 확인해보자\n",
    "dense_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 층에 포함되는 모든 파라미터(가중치, 편향)이 튜플 형태로 저장된 ‘parameters’\n",
    "# 이후, 모델을 학습시키는 과정에서 파라미터에 접근하는 것을 용이하게 해준다\n",
    "dense_layer.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23327386",
   "metadata": {},
   "source": [
    "이번에는 이 밀집층에 적절한 데이터를 입력시켜, 출력되는 값을 확인해보자. 각각의 차원이 2이고, batch 크기가 4인 test_data를 랜덤하게 생성해보자. 앞서 확인한 가중치와 편향 값으로 모델을 통해 계산되어야 하는 출력값을 예측해보고, 코드의 실행 결과와 일치하는지도 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef708fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤하게 데이터 생성\n",
    "test_data = np.random.rand(4, 2)\n",
    "# 생성한 데이터 출력\n",
    "print(test_data)\n",
    "\n",
    "# 밀집층에 의한 출력값 계산\n",
    "# 내부적으로 이루어지는 계산은 test_data @ weight + bias (브로드캐스팅 고려)\n",
    "# 예측되는 출력 결과는 shape-(4, 3) 텐서\n",
    "dense_layer(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf55e33",
   "metadata": {},
   "source": [
    "## Step 4. 사용할 활성함수, 손실함수와 모델 파라미터 초기화 방식 import 하기  \n",
    "  \n",
    "Step 3에서는 MyNN 라이브러리를 이용하여 밀집층을 구성하는 방법을 익혔다. 이를 활용하여 본격적인 신경망을 구현하기 이전에, 실습 6에서 사용했던 활성함수인 ReLU 함수와 모델 파라미터 초기화 방식인 he_normal을 동일하게 import 하자. 이 과정은 실습 6의 Step 3에 명시되어있다. 손실함수로 사용할, 소프트맥스 함수와 교차 엔트로피 함수가 결합된 softmax_crossentropy 함수도 import 하자. 이 과정은 실습 6의 Step 4에 명시되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a99da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2fce7",
   "metadata": {},
   "source": [
    "## Step 5. 모델(클래스) 정의하기  \n",
    "  \n",
    "이제, MyNN 라이브러리를 적절히 활용하여 우리의 모델을 클래스의 형태로 코딩해보자. 클래스의 기능은 실습 6에서와 동일하게, 모델 파라미터를 초기화하고, 순전파 과정을 시행하는 것이다. 전반적인 클래스의 구조는 이때까지 살펴본 실습들에서와 유사하다.  \n",
    "우리가 최종적으로 구현하고자 하는 신경망은 실습 6에서와 마찬가지로, 다음과 같은 근사식을 표현하는 것이다.  \n",
    "  \n",
    "$ \\mathbf{y_{i}} = F(\\{\\mathbf{v_i}\\}, \\{\\mathbf{w_i}\\}, \\{b_i\\}; \\mathbf{x}) = \\sum \\limits_{i=1}^N \\mathbf{v_i} \\varphi (\\mathbf{x} \\cdot \\mathbf{w_i} + b_i) $  \n",
    "  \n",
    "__init__ 메서드를 작성할 때 두 개의 밀집층을 생성하고, 이 둘을 이용하여 전체적인 순전파를 구성해보자. 거치게 되는 첫 번째 밀집층은 점의 좌표인 2개의 입력을 받아 사용할 뉴런의 개수만큼의 출력을 내도록 하자. 이를 통해 $\\mathbf{x} \\cdot \\mathbf{w_i} + b_i$의 연산을 구현하자. 이후, 계산된 결과(행렬) 전체에 활성함수인 ReLU 함수 $\\varphi$를 취하자. 이어, 뉴런의 개수만큼의 입력을 받아 최종적으로 원하는 결과인 3개(총 소용돌이의 개수)의 출력을 내보내는 두 번째 밀집층을 연결해주면 된다. 두 번째 밀집층의 경우 편향을 두지 않도록 하고, 이 모든 연산을 이어 진행하면 위의 근사식에서의 계산을 구현할 수 있다. 이제 아래 코드를 완성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8062890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, num_neurons, num_classes):\n",
    "        \"\"\"\n",
    "        모델에 포함되는 모든 층을 생성.\n",
    "        생성한 층들은 클래스의 속성으로 설정한다.\n",
    "        \n",
    "        매개변수 (Parameters)\n",
    "        ----------\n",
    "        num_neurons : int형\n",
    "            은닉층의 크기(은닉층에서 사용하게 될 뉴런의 개수)\n",
    "            \n",
    "        num_classes : int형\n",
    "            출력층의 크기(점을 분류하게 될 소용돌이의 총 개수)\n",
    "        \"\"\"\n",
    "        # 밀집층 dense1\n",
    "        # 점의 좌표, 즉 두 개의 데이터를 입력받아 은닉층 뉴런 개수만큼의 출력\n",
    "        # 가중치 초기화는 He-normal 초기화를 사용\n",
    "        # 편향 초기화는 default인 상수(0) 초기화를 사용\n",
    "        self.dense1 = dense(2, num_neurons, weight_initializer=he_normal)\n",
    "\n",
    "        # 밀집층 dense2\n",
    "        # 은닉층 뉴런 개수만큼의 데이터를 입력받아 소용돌이의 개수만큼의 출력\n",
    "        # 가중치 초기화는 He-normal 초기화를 사용\n",
    "        # 편향을 사용하지 않는 신경망\n",
    "        self.dense2 = dense(num_neurons, num_classes, weight_initializer=he_normal, bias=False)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        모델의 순전파를 진행\n",
    "        데이터를 입력받아 은닉층에서 출력층으로 전달하는 값을 계산\n",
    "        \n",
    "        매개변수 (Parameters)\n",
    "        ----------\n",
    "        x : Union[numpy.ndarray, mygrad.Tensor]형, shape=(M, 2)\n",
    "            M개의 데이터로 구성된 하나의 batch, 각 데이터(점의 좌표)의 차원은 2\n",
    "            \n",
    "        반환 값 (Returns)\n",
    "        -------\n",
    "        mygrad.Tensor형, shape=(M, num_classes)\n",
    "            신경망의 은닉층에서 출력층으로 전달하게 될 shape-(M, num_classes)인 행렬\n",
    "        \"\"\"\n",
    "        # self.dense1과 self.dense2, relu 함수를 적절히 사용하여 작성하기\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        모델 파라미터들에 대한 getter를 데코레이터를 이용하여 구현\n",
    "        이전의 실습들에서와 유사한 형태임을 참고\n",
    "        \n",
    "        반환 값 (Returns)\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            모델 파라미터인 dense1과 dense2의 모든 파라미터를 전부 묶은 튜플을 반환\n",
    "        \"\"\"\n",
    "        return self.dense1.parameters + self.dense2.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf033f",
   "metadata": {},
   "source": [
    "## Step 6. 모델 학습시키기  \n",
    "  \n",
    "이제 모델을 학습시켜보자. 우선, Step 5에서 구현한 클래스의 객체를 생성하자. 초기화 과정에서 뉴런의 개수는 15개, 분류 클래스의 개수는 Step 1에서 지정한 num_tendrils로 지정해주자. 뉴런의 개수는 물론 다른 숫자를 사용해도 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2252a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_neurons=15, num_classes=num_tendril)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc15dc49",
   "metadata": {},
   "source": [
    "이번에는 경사하강법을 직접 코딩하여 구현하는 대신, MyNN 라이브러리에서 지원하는 확률적 경사하강법(SGD, Stochastic Gradient Descent)라는 optimizer를 사용해볼 것이다. SGD는 그 이름으로부터 짐작해볼 수 있듯, 경사하강법의 한 종류이다. 이때까지 우리가 사용했던 경사하강법은 학습용 데이터셋 전체에 대해 손실함수를 계산하여 기울기를 검토한 방법이었다. 이 방법은 데이터의 개수가 많아짐에 따라 계산량이 늘어나 학습에 걸리는 시간이 길어진다는 문제가 있고, 이를 해결하기 위해 한 번에 데이터 한 개를 랜덤으로 선택하여, 이에 대한 손실함수를 계산하고 매번 모델 파라미터를 갱신해주는 방법이 바로 SGD이다. optimizer로 SGD를 사용하면, 학습용 데이터셋을 이루는 데이터 각각에 대해 한 번씩 갱신을 진행하고, 모든 데이터에 대한 갱신이 완료되면 데이터들의 순서를 바꾸어 갱신을 진행하는 것을 반복하게 된다.  \n",
    "  \n",
    "아래는 SGD optimizer의 클래스를 import하고, 객체를 생성하는 코드이다. 객체 생성 과정에서 학습률은 0.1로 초기화하였다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.optimizers.sgd import SGD\n",
    "optim = SGD(model.parameters, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c179efa",
   "metadata": {},
   "source": [
    "학습에 사용될 batch_size는 50으로 설정하자. 이 값과 학습률 등의 하이퍼파라미터는 적절히 다른 값을 사용해보아도 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a8bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d963c91",
   "metadata": {},
   "source": [
    "학습을 진행하며 손실함수 값과 분류 정확도 값을 실시간으로 그래프로 나타내기 위해, 앞선 실습들에서 사용해보았던 noggin 라이브러리를 사용할 것이다. 아래 코드를 적절히 작성하여 두 그래프를 그릴 좌표평면을 만들자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b0849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noggin 라이브러리로부터 create_plot 함수를 import\n",
    "# 여기에 코드 작성\n",
    "# 그래프를 그려나갈 공간 생성\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c6f95",
   "metadata": {},
   "source": [
    "이제, 아래의 주석을 천천히 읽으며 학습을 진행시키는 코드를 완성해보자. 실습 6에서와 마찬가지로, batch를 구성할 때 우리의 학습 데이터셋을 순서대로 묶지 않고, 한 번 무작위하게 순서를 바꾸어준 후 묶어주어야 학습이 잘 진행된다. 이번에는 MyNN 라이브러리를 사용하기에 우리가 직접 구현해주어야 하는 부분이 줄어든 것을 확인할 수 있을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c09ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_cnt in range(5000):\n",
    "    # 한 번의 epoch가 반복될 때마다 시행하는 코드\n",
    "    # 학습용 데이터셋을 무작위로 재배열하기\n",
    "    # `idxs`를 무작위로 재배열하여 데이터셋의 인덱스를 어떻게 재배열할지 결정\n",
    "    idxs = np.arange(len(xtrain))  # -> array([0, 1, ..., 9999])\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    for batch_cnt in range(0, len(xtrain) // batch_size):\n",
    "        # (총 학습용 데이터셋 개수)를 (batch 크기)로 나눈 몫만큼 반복하며 시행\n",
    "\n",
    "        # 학습용 데이터셋의 새로운 인덱스를 슬라이싱하여 그 배열을 batch_indices에 저장\n",
    "        # 어느 범위까지 잘라야 할지 생각해보기\n",
    "        # 힌트: 이번 for문의 iteration number인 batch_cnt를 활용\n",
    "        # 여기에 코드 작성\n",
    "\n",
    "        # 학습에 사용할 batch를 추출하기 위해, 윗 단계에서 저장한 batch_indices 위치의 데이터들을 모아 batch와 truth를 생성\n",
    "        batch = xtrain[batch_indices]\n",
    "        truth = ytrain[batch_indices]\n",
    "\n",
    "        # 모델의 예측값(은닉층에서 출력층으로 전달되는 값) 구하기\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        # 대응되는 손실함수 값 구하기, softmax_crossentropy 함수 활용\n",
    "        # 역전파를 진행하기 위해, 결과값은 tensor 형이어야 함\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        # mygrad tensor의 자동미분 함수를 활용하여 역전파 진행\n",
    "        # 이 과정은 MyNN 라이브러리를 사용하는 경우에도 직접 해 주어야 함\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        # 미리 생성한 optimizer 객체를 이용하여, 모델 파라미터를 한 차례 갱신\n",
    "        optim.step()\n",
    "        \n",
    "        # Step 2에서 구현한 accuracy 함수를 이용하여 정확도 계산, acc라는 이름으로 저장\n",
    "        # 여기에 코드 작성\n",
    "\n",
    "        plotter.set_train_batch({\"loss\" : loss.item(),\n",
    "                                 \"accuracy\" : acc},\n",
    "                                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b02087",
   "metadata": {},
   "source": [
    "코드를 잘 작성하였다면, 실습 6에서와 마찬가지로 손실함수 값은 감소하고 분류 정확도 값은 증가하는 양상이 확인될 것이다. 정확도를 보며 학습이 잘 이루어졌는지 판단해보자. 추가적으로, 실습 6의 마지막 부분에서 학습된 모델이 점을 분류하는 경계를 색으로 시각화한 그림을 그리는 아래의 코드 또한 그대로 실행해보자. 소용돌이가 잘 분류된다면, 우리는 실습 6에서 직접 구현하던 신경망 학습을 MyNN 라이브러리를 활용하여 성공적으로 마친 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125bdb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_function(x):\n",
    "    from mygrad.nnet.activations import softmax\n",
    "    return softmax(model(x)).data\n",
    "\n",
    "fig, ax = data.visualize_model(dummy_function, entropy=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e84972",
   "metadata": {},
   "source": [
    "## 배운 내용 되돌아보기  \n",
    "  \n",
    "이번 실습에서는 실습 6에서의 문제를 MyNN 라이브러리를 활용하여 다시 한번 해결해 보았다. MyNN 라이브러리는 앞으로의 실습에서도 활용될 예정이며, PyTorch 등 딥러닝을 위해 개발된 다른 라이브러리 사용에 있어서도 도움을 줄 수 있으니 이번 실습에서 배운 라이브러리의 기능들을 잘 기억해두자. 다음 절로 넘어가기 전에, 배운 내용들을 한 번씩 되짚어보자.  \n",
    "  \n",
    "- MyNN 라이브러리의 dense 클래스를 사용하여, 원하는 크기의 입력으로부터 원하는 크기의 출력을 만들어낼 수 있는 밀집층을 생성하는 방법을 배웠다.\n",
    "- 밀집층을 생성할 때, 가중치와 편향의 초기화 방법 등을 선택하는 방법을 배웠다.\n",
    "- 생성한 밀집층에 적절한 크기의 입력 데이터를 넣어, 그로부터 계산되어 출력되는 데이터를 계산해보았다.\n",
    "- MyNN 라이브러리의 SGD 클래스를 사용하여, 직접 지정한 학습률을 이용하여 확률적 경사하강법(SGD, Stochastic Gradient Descent)을 시행하는 optimizer 객체를 생성하였다.\n",
    "- 손실함수에 대해 역전파를 진행한 이후, 생성한 optimizer 객체를 이용하여 모델 파라미터를 갱신해보았다.\n",
    "- 이러한 과정들을 적절히 활용하여 점이 속하는 소용돌이를 분류하는 문제를 해결하는 신경망을 학습시켰고, 그 결과 분류가 잘 일어나는 것을 확인하였다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
