{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b98eb68",
   "metadata": {},
   "source": [
    "# 실습 9: CNN을 이용한 MNIST 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16b819c",
   "metadata": {},
   "source": [
    "본 실습에서 사용하는 데이터셋(MNIST)의 출처는 다음과 같음을 밝힌다.\n",
    "\n",
    "Raw Data 출처: Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \"Gradient-based learning applied to document recognition.\", Proceedings of the IEEE, 86(11):2278-2324, November 1998.\n",
    "\n",
    "편집자1: Yann LeCun, Corinna Cortes, Christopher JC Burges, 《THE MNIST DATABASE of handwritten digits》, 〈http://yann.lecun.com/exdb/mnist/〉, 방문 날짜 2022.03.01.\n",
    "\n",
    "편집자2: MIT Beaverworks에서 편집자1의 웹사이트의 데이터셋을 쉽게 다운 받을 수 있는 라이브러리 datasets를 제작"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff842dd",
   "metadata": {},
   "source": [
    "이번 실습에서는 유명한 데이터셋인 MNIST를 이용하여 CNN을 학습시킬 것이다. MNIST 데이터셋은 0부터 9까지의 숫자를 손으로 쓴 이미지와 그에 대응되는 숫자 라벨로 이루어져 있는 데이터셋이다. 학습용 데이터셋은 60,000개의 이미지로, 테스트용 데이터셋은 10,000개의 이미지로 이루어져 있다. 따라서 MNIST 데이터셋을 이용하여 CNN을 학습시키면, 0부터 9까지의 숫자를 손으로 쓴 글씨 이미지들을 분류하는 신경망 모델을 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b64951",
   "metadata": {},
   "source": [
    "이번 실습에서 필요한 기본적인 라이브러리들을 먼저 import 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33519a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mygrad as mg\n",
    "from mygrad import Tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd5a33",
   "metadata": {},
   "source": [
    "### Step 1. MNIST 데이터 관찰하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1cf4e",
   "metadata": {},
   "source": [
    "셋업을 잘 진행했다면, 현재 주피터 노트북 파일이 들어있는 폴더의 상위폴더에 Datasets 폴더가 이미 있을 것이다. 먼저 mnist.npz 데이터셋이 Datasets 폴더에 잘 다운로드되어 있는지 확인해보자. 없다면 첫번째 실습이었던 셋업이 잘 이루어지지 않은 것이므로 다시 다운받아야 한다.\n",
    "\n",
    "MNIST 데이터셋을 다운로드하는 다른 방법으로는 아래 코드를 이용하여 데이터셋을 다운로드하는 방법이 있다. 셋업 단계에서 패키지를 설치할 때 깃허브에서 datasets 패키지를 설치했었기 때문에, 아래 코드만으로 다운로드할 수도 있다. 이미 정확한 위치에 제대로 다운로드되어 있다면 아래 코드를 실행했을 때 File already exists: 라는 메시지가 출력될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f486dd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from datasets import load_mnist, download_mnist\n",
    "download_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a2188",
   "metadata": {},
   "source": [
    "MNIST 데이터셋을 구성하는 학습용 데이터셋과 테스트용 데이터셋을 가져오자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e891b5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# mnist 데이터셋을 불러와서 학습용/테스트용 이미지/라벨 을 저장\n",
    "x_train, y_train, x_test, y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f566f3b",
   "metadata": {},
   "source": [
    "x_train, y_train, x_test, y_test는 모두 numpy.ndarray 이다. 각각의 shape과 데이터 타입(dtype)을 확인해보자. 또한, x_train과 x_test의 shape으로부터 데이터셋을 구성하는 이미지가 각각 몇 개의 색상 채널로 이루어져 있는지 확인하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ffb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_train), x_train.shape, x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train), y_train.shape, y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_test), x_test.shape, x_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754cbbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_test), y_test.shape, y_test.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c283c88b",
   "metadata": {},
   "source": [
    "이제 MNIST 데이터셋이 어떤 데이터로 구성되어 있는지 확인하기 위해 아래 코드를 사용해보자. matplotlib을 이용하여 x_train의 이미지를 띄우고, 제목으로 y_train의 truth 값을 출력할 것이다. img_id 값을 바꾸어가며 MNIST 데이터셋을 이루는 데이터들을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b78393",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "img_id = 5\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(x_train[img_id, 0], cmap=\"gray\")\n",
    "ax.set_title(f\"truth: {y_train[img_id]}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd4b203",
   "metadata": {},
   "source": [
    "### Step 2. 학습의 큰 틀 구상하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54874763",
   "metadata": {},
   "source": [
    "신경망의 학습을 이용하여 문제를 해결하려 할 때 다음 질문들에 답해야 한다.\n",
    "\n",
    "> 1. 우리가 해결하려고 하는 문제의 목적은 무엇인가?\n",
    ">\n",
    "> 2. 이 문제를 해결하기 위해 어떤 데이터셋이 필요한가? 어떻게 마련해야 할까?\n",
    ">\n",
    "> 3. 데이터를 신경망에 넣어주기 위해 어떤 전처리가 필요할까?\n",
    ">\n",
    "> 4. 이 문제를 해결하기 위해 어떤 신경망을 쓰는 게 좋을까? 각 계층을 어떻게 구성해야 할까?\n",
    ">\n",
    "> 5. 어떤 손실함수를 선택해야 할까? 정확도함수는 어떻게 정의할 수 있을까?\n",
    ">\n",
    "> 6. 어떤 Optimizer를 선택해야 할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f066df6",
   "metadata": {},
   "source": [
    "하나씩 대답하면서 학습의 큰 틀을 구상해보자. (1번, 2번 질문에 대해서는 이미 대답을 알고있지만, 새로운 문제에 직면했다고 생각하고 따라가 보자.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e9c2fd",
   "metadata": {},
   "source": [
    "**Q1.** 우리가 해결하려고 하는 문제의 목적은 무엇인가?\n",
    "\n",
    "A1. 0부터 9까지의 숫자를 손으로 쓴 글씨 이미지들을 분류하는 문제를 해결하고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146096c",
   "metadata": {},
   "source": [
    "**Q2.** 이 문제를 해결하기 위해 어떤 데이터셋이 필요한가? 어떻게 마련해야 할까?\n",
    "\n",
    "A2. Step 1에서 살펴본 MNIST 데이터셋은 우리가 해결하려는 문제에 적합한 데이터셋이다. 따라서 이를 학습에 이용하면 될 것이다. 만약 이러한 데이터셋이 세상에 존재하지 않거나 공개되어 있지 않았다면, 데이터셋을 직접 만드는 데 오랜 시간이 걸렸을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63fc9e",
   "metadata": {},
   "source": [
    "**Q3.** 데이터를 신경망에 넣어주기 위해 어떤 전처리가 필요할까?\n",
    "\n",
    "A3. 그동안의 실습에서 진행했던 전처리와 유사하게, 데이터 타입을 바꾸는 작업과 데이터에 대한 특성 스케일링(정규화)을 진행하면 된다. 그리고 5절 이론에서 배운 패딩(padding)을 적용해주는 전처리도 추가적으로 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad6e33",
   "metadata": {},
   "source": [
    "**Q4.** 이 문제를 해결하기 위해 어떤 신경망을 쓰는 게 좋을까? 각 계층을 어떻게 구성해야 할까?\n",
    "\n",
    "A4. 이미지에 관한 학습이므로 합성곱 계층과 최대 풀링 계층을 사용한 CNN을 사용하는 것이 좋다. 우리는 합성곱 계층과 풀링 계층으로 구성된 은닉층 두 개와 밀집층으로 구성된 은닉층 하나, 밀집층으로 구성된 출력층 한 개로 이루어진 4층 CNN을 사용해볼 것이다. 활성함수는 모든 은닉층에서 ReLU이고, 출력층에서는 Softmax 함수이다.\n",
    "\n",
    "```\n",
    "CONV -> RELU -> POOL -> CONV -> RELU -> POOL -> FLATTEN -> DENSE -> RELU -> DENSE -> SOFTMAX\n",
    "```\n",
    "\n",
    "우리는 분류 문제를 해결해야 하므로, 입력 데이터가 각 분류 항목에 해당할 확률을 항목별 인덱스에 대응되는 배열로 얻기를 바란다. 즉, 입력된 이미지가 숫자 $i$일 확률을 $p_i$라 할 때 출력층에서는 $[p_0, p_1, p_2, \\cdots, p_9]$가 출력되도록 할 것이다. 따라서 각 뉴런의 출력값이 0과 1 사이의 값을 갖고 합은 1이 되는 조건을 만족할 수 있도록, 출력층의 활성함수를 Softmax 함수로 선택하는 것이 좋은 선택임을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef3168",
   "metadata": {},
   "source": [
    "**Q5.** 어떤 손실함수를 선택해야 할까? 정확도함수는 어떻게 정의할 수 있을까?\n",
    "\n",
    "A5. 분류 문제에서 유용하게 쓰이는 교차 엔트로피 함수를 손실함수로 사용하자. 그리고 성능을 평가하기 위한 지표로 손실(loss)과 더불어 정확도(accuracy)를 사용할 것이다. 분류 문제이므로 정확도는 맞으면 1, 틀리면 0으로 하여 평균을 계산하면 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca8785",
   "metadata": {},
   "source": [
    "**Q6.** 어떤 Optimizer를 선택해야 할까?\n",
    "\n",
    "A6. 경사하강법 다음으로 기본적인 Optimizer인 SGD를 사용하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc17f1",
   "metadata": {},
   "source": [
    "### Step 3. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5257f",
   "metadata": {},
   "source": [
    "진행해야 하는 데이터 전처리는 크게 세가지이다.\n",
    "\n",
    "1) 데이터 타입을 신경망에서 사용할 수 있도록 float으로 바꾸어준다.\n",
    "\n",
    "2) 데이터에 대한 특성 스케일링(정규화)을 진행한다.\n",
    "\n",
    "3) 첫번째 합성곱 연산 후에 이미지의 shape이 유지되도록 패딩을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7e376a",
   "metadata": {},
   "source": [
    "먼저, 원본 이미지에는 각 픽셀에 해당하는 값으로 부호 없는 8비트 정수(uint8)가 저장되어있다. 그러나 우리는 신경망 각 계층의 연산을 수행하기 위해 이를 float형으로 변환해야 한다. .astype()을 사용하여 배열 x_train과 x_test의 데이터 타입을 np.float32로 바꾸어주자. (라벨인 y_train과 y_test의 데이터 타입은 상관이 없다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f222ee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3362f4",
   "metadata": {},
   "source": [
    "두번째로, 우리는 학습이 잘 진행되도록 하기 위해 이미지 데이터들의 특성 스케일링(정규화)을 진행해줄 것이다. 각 픽셀 값은 [0, 255]의 범위로 경계가 분명하게 고정되어 있기 때문에, 최소-최대 정규화를 사용하기 적절하다. 그런데 이 실습에서는 최솟값과 최댓값이 각각 0(검은색)과 255(흰색)이다. 따라서 모든 픽셀 값을 255로 나누어주기만 하면 최소-최대 정규화를 통해 그 값이 [0, 1]의 범위로 바뀌도록 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ff406",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47e117",
   "metadata": {},
   "source": [
    "마지막으로, 이미지 둘레에 패딩을 도입해보자. shape 5x5인 필터를 스트라이드 1로 이용할 것이므로, 첫번째 합성곱 연산 후에 이미지의 shape이 유지되도록 하기 위해서는 이미지의 모든 면에 0으로 이루어진 두 개의 행/열을 패딩해주면 된다. 우리가 갖고 있는 원래의 이미지가 28x28이므로, 패딩 후의 이미지는 32x32가 될 것이다. \n",
    "\n",
    "참고로, 패딩의 폭은 하이퍼파라미터로, 합성곱 계층을 정의할 때 그 값을 지정해주기도 한다. 하지만 이 실습에서는 학습 전 전처리 작업으로 패딩을 도입하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8ede7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "x_train = np.pad(x_train, ((0, 0), (0, 0), (2, 2), (2, 2)), mode=\"constant\")\n",
    "x_test = np.pad(x_test, ((0, 0), (0, 0), (2, 2), (2, 2)), mode=\"constant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f91aef9",
   "metadata": {},
   "source": [
    "### Step 4. 모델(클래스) 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d51feb",
   "metadata": {},
   "source": [
    "#### 가중치 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84617d8",
   "metadata": {},
   "source": [
    "우리에게 이제 클래스로 신경망 모델을 정의하는 것 쯤은 전혀 어렵지 않은 일이다. 바로 프로그래밍을 하고 싶겠지만, 그 전에 가중치를 초기화하는 방법에 관해 하나만 더 알아보자.\n",
    "\n",
    "우리는 그동안 가중치를 초기화하기 위해 다양한 initializer (uniform, normal, he_normal)를 사용하였다. 이번에는 가중치를 초기화하기 위해 Xavier Glorot이 Yoshua Bengio와 함께 작성한 [논문](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) 에서 제안한 **자비에 초기화(Xavier Initialization)** 방법 중, **glorot_uniform**을 initializer로 이용할 것이다. 편향은 0으로 초기화할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add6b55",
   "metadata": {},
   "source": [
    "glorot_uniform 초기화 방식은 다음의 균등분포를 따르도록 초기화하는 것이다. $N_{in}$과 $N_{out}$은 각각 들어오는 입력 수와 내보내는 출력 수이다. 핵심은 들어오는 쪽의 뉴런 수만 고려했던 He 초기화(He Initialization)와 달리 내보내는 쪽도 함께 고려한다는 점이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba1f5b",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "W \\sim U(-\\frac{\\sqrt 6}{\\sqrt{N_{in}+N_{out}}},+\\frac{\\sqrt 6}{\\sqrt{N_{in}+N_{out}}})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a7ec7",
   "metadata": {},
   "source": [
    "mygrad.nnet.initializers의 자비에 초기화 함수들(glorot_normal, glorot_uniform)과 He 초기화 함수들(he_normal, he_uniform)에는 모두 “gain”이라는 매개변수(parameter)가 정의되어 있다. gain은 가중치 배열에 곱해지는 스케일링 인자(상수배 해주는 역할. scaling factor)이며, 디폴트 값으로 1을 가진다. 이 실습에서는 gain을 $\\sqrt{2}$로 사용할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7fb0e0",
   "metadata": {},
   "source": [
    "가중치 초기화 방법 (weight_initializer)은 밀집층 혹은 합성곱 계층을 초기화할 때 아래 코드와 같이 지정해주면 된다. 이때, weight_kwargs에 weight_initializer가 필요로 하는 모든 매개변수들을 딕셔너리 형태로 전달해줄 수 있다. 이 실습에서는 gain을 $\\sqrt2$로 하는 것 외에 다른 매개변수는 필요로 하지 않으므로 gain만 전달해주면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad1a04",
   "metadata": {},
   "source": [
    "```python\n",
    "from mygrad.nnet.initializers import glorot_uniform\n",
    "\n",
    "gain = {'gain': np.sqrt(2)}\n",
    "\n",
    "# conv layer, dense layer 등을 초기화할 때 다음과 같이 사용\n",
    "dense(d1, d2, \n",
    "      weight_initializer=glorot_uniform, \n",
    "      weight_kwargs=gain)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed1f10",
   "metadata": {},
   "source": [
    "#### 모델(클래스) 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78602e62",
   "metadata": {},
   "source": [
    "이제 위에서 구상했던 계층 정보대로 신경망 모델을 클래스로 작성하면 된다. 우리가 구성할 신경망은 아래와 같이 두 개의 합성곱-풀링 계층과 두 개의 밀집층으로 이루어진 4층 신경망이다.\n",
    "\n",
    "계층을 어떻게 구성하면 좋을지 각 계층의 하이퍼파라미터들을 결정해보자. 학습이 괜찮게 이루어지는 각 계층의 정보는 다음과 같다. (하이퍼파라미터로 다양한 값들을 시도해보아도 좋다.)\n",
    "\n",
    "> 계층1_CONV: 5x5필터 20개, 스트라이드-1\n",
    ">\n",
    "> 계층1_POOL: 2x2, (스트라이드-2)\n",
    ">\n",
    "> 계층2_CONV: 5x5필터 10개, 스트라이드-1\n",
    ">\n",
    "> 계층2_POOL: 2x2, (스트라이드-2)\n",
    ">\n",
    "> 계층3_DENSE: 뉴런 20개\n",
    ">\n",
    "> 계층4_DENSE: 뉴런 10개 (**주의**: $[p_0, p_1, p_2, \\cdots, p_9]$ 가 출력되어야 한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e007b5f",
   "metadata": {},
   "source": [
    "위 계층 정보 중 필터와 풀링 윈도우의 shape 및 스트라이드는 클래스를 정의하는 단계에서 사용해야 하는 정보이고, 나머지 정보인 필터의 개수와 뉴런의 개수는 클래스로부터 객체를 생성하여 모델을 초기화할 때 지정해줄 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c6d6f8",
   "metadata": {},
   "source": [
    "MyNN의 conv(), dense() 함수를 사용하여 계층을 생성할 때 전달해주어야 하는 매개변수가 무엇인지 알아보자.\n",
    "\n",
    "일단 dense() 함수는 앞선 다른 실습들에서도 살펴보았듯 input_size(입력 데이터 수)와 output_size(출력 수 = 뉴런 수)를 전달해주어야 하고, 위에서와 같이 glorot_uniform 초기화를 위해 weight_initializer와 weight_kwargs를 지정해줄 수 있다.\n",
    "\n",
    "그리고 conv() 함수는 input_size(입력 채널 수)와 output_size(출력 채널 수), filter_dims(필터 size)를 전달해주어야 하고, dense()와 마찬가지 방법으로 glorot_uniform 초기화를 진행할 수 있다. 특히 filter_dims는 전달 가능한 개수가 정해지지 않은 \\*args 형태이다. 이 실습의 경우 $5 \\times 5$ 필터를 사용하므로, 5,5를 전달해주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bba8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.layers.conv import conv\n",
    "from mynn.layers.dense import dense\n",
    "\n",
    "from mygrad.nnet.initializers import glorot_uniform\n",
    "from mygrad.nnet.activations import relu\n",
    "from mygrad.nnet.layers import max_pool\n",
    "\n",
    "class Model:\n",
    "    ''' 간단한 CNN (Convolutional Neural Network) '''\n",
    "    \n",
    "    def __init__(self, num_input_channels, f1, f2, d1, num_classes):\n",
    "        \"\"\"\n",
    "        클래스 생성 시 실행되어 계층과 가중치들을 초기화하는 역할.\n",
    "        \n",
    "        매개변수 (Parameters)\n",
    "        ----------\n",
    "        num_input_channels : int\n",
    "            입력 데이터(이미지)의 색상 채널 수\n",
    "            \n",
    "        f1 : int\n",
    "            첫번째 계층 _ 합성곱 계층의 필터 수\n",
    "        \n",
    "        f2 : int\n",
    "            두번째 계층 _ 합성곱 계층의 필터 수\n",
    "\n",
    "        d1 : int\n",
    "            세번째 계층 _ 밀집층의 뉴런 수\n",
    "        \n",
    "        num_classes : int\n",
    "            모델의 분류 항목 수\n",
    "            네번째 계층 _ 밀집층의 뉴런 수\n",
    "        \"\"\"\n",
    "        # 두 개의 합성곱 계층과 두 개의 밀집층을 각각 초기화\n",
    "        # MyNN의 conv(), dense() 함수 사용\n",
    "        # weight_initializer = glorot_uniform, gain = np.sqrt(2) 로 설정\n",
    "        \n",
    "        # 주의: 두 번째 계층의 계산 결과로 얻은 이미지 채널을 벡터화(vectorization, flatten)한 결과가\n",
    "        # 세 번째 계층으로 들어가므로, 세 번째 계층의 밀집층의 input_size를 잘 계산해야 함\n",
    "        \n",
    "        # 여기에 코드 작성\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        입력 데이터에 따른 모델의 순전파(Forward Propagation)를 수행\n",
    "        \n",
    "        매개변수 (Parameters)\n",
    "        ----------\n",
    "        x : numpy.ndarray, shape=(N, 1, 32, 32)\n",
    "            입력 데이터. N장의 이미지\n",
    "            \n",
    "        반환 값 (Returns)\n",
    "        -------\n",
    "        mygrad.Tensor, shape=(N, num_classes)\n",
    "            N장의 이미지에 대해 분류 항목별 예측값을 배열로 반환\n",
    "            Softmax 통과 전이므로 확률이 아님에 주의\n",
    "        '''\n",
    "        # 모델의 구조를 잘 생각하며 모델의 순전파를 수행하는 코드 작성\n",
    "        # 가중치를 포함하는 합성곱 계층 및 밀집층은 __init__에서 정의함\n",
    "        # 활성함수 relu와 최대 풀링 함수 max_pool을 추가로 이용하여 작성하면 됨\n",
    "        \n",
    "        # 이미지 채널을 밀집층에 넣어주기 위한 벡터화 진행 시,\n",
    "        # N개의 이미지에 대해 개별적으로 벡터화를 진행해야 함 (HINT > NumPy ndarray의 reshape() 메서드 사용) \n",
    "        \n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        모델 파라미터를 리스트 형태로 전부 가져올 수 있는 유용한 함수\n",
    "        데코레이터 @property를 붙였기에 메서드가 아닌 속성처럼 사용해야 함.\n",
    "        즉, model.parameters()가 아닌 model.parameters로 호출\n",
    "        \n",
    "        반환 값 (Returns)\n",
    "        -------\n",
    "        List[Tensor, ...]\n",
    "            모델의 학습 가능한 모델 파라미터들을 모아놓은 리스트\n",
    "        \"\"\"\n",
    "        # __init__에서 정의한 4개의 계층에 포함된 모든 가중치(모델 파라미터) 반환\n",
    "        # 각 레이어의 파라미터를 self.conv.parameters와 같이 가져올 수 있음\n",
    "        # for문 이용하기\n",
    "        \n",
    "        # 여기에 코드 작성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397fe02",
   "metadata": {},
   "source": [
    "### Step 5. 정확도 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f9764",
   "metadata": {},
   "source": [
    "분류의 정확도를 출력하는 함수를 작성하자. 정확도는 맞으면 1, 틀리면 0으로 하여 평균을 계산하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, truth):\n",
    "    \"\"\"\n",
    "    하나의 batch에 대한 모델의 예측값과 실제값을 비교하여, 정확도를 계산하는 함수\n",
    "    \n",
    "    매개변수 (Parameters)\n",
    "    ----------\n",
    "    predictions : Union[numpy.ndarray, mg.Tensor], shape=(M, D)\n",
    "        D개의 분류 항목에 대해 예측한 각 확률을 batch의 데이터 M개에 대해 배열로 정리\n",
    "        이 실습에서는 분류할 항목의 개수가 숫자 0~9, 총 10개이므로 D = 10\n",
    "        \n",
    "    truth : numpy.ndarray, shape=(M,)\n",
    "        데이터 M개에 대응되는 정답 라벨들로 이루어진 배열\n",
    "        각 라벨은 D개의 분류 항목 [0, D) 중 하나의 값\n",
    "            \n",
    "    반환 값 (Returns)\n",
    "    -------\n",
    "    float\n",
    "        올바른 예측의 비율 (0 이상 1 이하의 실수값)\n",
    "        해당 batch에 대한 모델의 분류 정확도\n",
    "    \"\"\"\n",
    "    # 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100e7fc",
   "metadata": {},
   "source": [
    "### Step 6. 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7f0c5",
   "metadata": {},
   "source": [
    "앞서 구상 단계에서 우리는 손실함수는 교차 엔트로피 함수, Optimizer는 MyNN의 SGD로 선택하기로 하였다. 학습을 시키는 단계 또한 앞서 여러 차례 프로그래밍 해봤던 부분이므로 잘 할 수 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a82f5",
   "metadata": {},
   "source": [
    "#### 모델 및 Optimizer 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126dd319",
   "metadata": {},
   "source": [
    "먼저 Optimizer를 MyNN의 SGD로 선택하여 모델을 초기화하자. Optimizer의 파라미터, 학습률(learning_rate) 외에 가중치 감소(weight_decay)와 모멘텀(momentum)이라는 값을 추가할 것이다.\n",
    "\n",
    "먼저 가중치 감소는 과적합(overfitting)을 방지하기 위한 방법 중 하나이다. 가중치의 값이 크면 과적합이 발생하는 경우가 많기 때문에, 가중치 값이 클수록 큰 페널티를 부과함으로써 가중치가 커지는 것을 방해한다. 이번 실습에서는 가중치 감소의 정도를 결정하는 하이퍼파라미터 값을 $5\\times10^{-4}$로 설정하면 적당하다.\n",
    "\n",
    "두 번째로 모멘텀은 물리에서의 ‘관성’을 가중치 업데이트에 반영한 것이다. 가중치가 기존에 움직이던 속도에 비례하여 해당 방향으로 계속 이동하도록 하는 힘이 작용한다고 이해하면 된다. 이를 통해 편미분 계수가 급격히 달라지더라도 좀 더 부드럽게 움직이도록 할 수 있다. 이번 실습에서는 기존에 움직이던 정도를 얼마나 반영할 것인지에 관한 하이퍼파라미터 값을 0.9로 설정하면 적당하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD를 import하여\n",
    "# Optimizer로 모델을 초기화\n",
    "\n",
    "from mynn.optimizers.sgd import SGD\n",
    "\n",
    "model = Model(f1=20, f2=10, d1=20, num_input_channels=1, num_classes=10)\n",
    "optim = SGD(model.parameters, learning_rate=0.01, momentum=0.9, weight_decay=5e-04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dbb2b6",
   "metadata": {},
   "source": [
    "#### 손실 및 정확도의 변화를 실시간으로 나타낼 그래프 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb8251",
   "metadata": {},
   "source": [
    "앞선 다른 실습에서 했던 바와 같이, 학습이 진행됨에 따라 동적인 그래프로 손실과 정확도를 확인할 수 있도록 아래의 코드를 실행하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from noggin import create_plot\n",
    "\n",
    "plotter, fig, ax = create_plot([\"loss\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0712658",
   "metadata": {},
   "source": [
    "#### 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3203600",
   "metadata": {},
   "source": [
    "batch size가 100인 batch들을 랜덤으로 구성하여, 2 epoch만큼 학습을 진행해보자. 이제껏 진행했던 학습 중 가장 데이터 수가 많고 깊이도 깊기 때문에 2 epoch만 진행함에도 시간이 꽤 걸릴 것이다. 그리고 더 좋은 학습 결과를 얻으려면 2 epoch로는 부족하고, 더 많은 학습이 필요할 것이라 짐작해볼 수 있다.\n",
    "\n",
    "학습용 데이터셋으로부터 랜덤 batch를 생성하여 학습을 진행하는 코드는 이전에 여러 번 작성해본 바 있으므로 잘 작성할 수 있을 것이다. 여기서 한 가지 추가해볼 부분은 학습용 데이터셋에 대한 1 epoch가 끝날 때마다 테스트용 데이터셋에 대해 예측을 진행하고 테스트 정확도를 판단하는 부분이다. 이 과정은 모델이 처음 접한 데이터에 대해서도 잘 작동하는지 확인하기 위한 중요한 과정이다.\n",
    "\n",
    "테스트용 데이터셋에 대해서도 batch를 나누어 학습의 정확도를 구한다. 이 부분에서는 가중치의 업데이트를 위한 역전파가 이루어지지 않는다. 또한 테스트용 데이터셋에 대한 정확도도 plotter로 나타내볼 수 있는데, 이때 테스트용 데이터셋에 대한 plotter.set_test_batch() 함수는 추적한 정확도를 일일이 그래프로 나타내지 않는다. 대신 잘 저장하고 있다가 plotter.set_test_epoch()에서 1 epoch 전체에 대한 평균 정확도를 구하여 그래프에 표시해준다. plotter.set_train_epoch()과 함께 진행함으로써 값을 비교할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e13e08",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for epoch_cnt in range(2):\n",
    "    \n",
    "    # x_train 데이터셋의 랜덤 batch 구성하기 위해\n",
    "    # 랜덤으로 섞인 idxs 배열 구성하기\n",
    "    \n",
    "    # 여기에 코드 작성\n",
    "    \n",
    "    \n",
    "    # 학습용 데이터셋의 batch들에 대해 학습을 진행 (앞선 실습들과 매우 유사함)\n",
    "    # 주의 1: 손실함수로 softmax_crossentropy() 이용\n",
    "    # 주의 2: 경사하강법을 진행할 때에는 Optimizer optim의 step() 실행\n",
    "    # 주의 3: 학습 중 loss와 accuracy를 추적\n",
    "    \n",
    "    for batch_cnt in range(len(x_train)//batch_size):\n",
    "        \n",
    "        # 여기에 코드 작성\n",
    "    \n",
    "    \n",
    "    # 테스트용 데이터셋의 batch들에 대해 모델을 평가\n",
    "    \n",
    "    for batch_cnt in range(0, len(x_test)//batch_size):\n",
    "        \n",
    "        # 학습 시와 마찬가지로 같은 batch size의 랜덤 batch를 구성\n",
    "        \n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        \n",
    "        # 역전파가 필요없는 부분이므로, 그래디언트 계산이 이뤄지지 않도록\n",
    "        with mg.no_autodiff:\n",
    "            # prediction : 테스트 batch에 대한 예측값\n",
    "            prediction = model(batch)\n",
    "            \n",
    "            # truth : 테스트 batch에 대한 실제값\n",
    "            truth = y_test[batch_indices]\n",
    "            \n",
    "            # accuracy: 앞서 작성한 accuracy() 이용한 정확도 계산\n",
    "            acc = accuracy(prediction, truth)\n",
    "        \n",
    "        # noggin으로 test-accuracy 추가\n",
    "        plotter.set_test_batch({\"accuracy\": acc}, batch_size=batch_size)\n",
    "    \n",
    "    plotter.set_train_epoch()\n",
    "    plotter.set_test_epoch()\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d5a01",
   "metadata": {},
   "source": [
    "### Step 7. 학습 결과 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d0052",
   "metadata": {},
   "source": [
    "학습 결과를 확인하기 위해, 테스트용 데이터셋에 대해 이미지와 그에 대응되는 모델의 예측 결과를 시각화해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, img_test, label_test = load_mnist()\n",
    "\n",
    "# MNIST의 라벨로 이루어진 튜플.\n",
    "# ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "labels = load_mnist.labels  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42f96e",
   "metadata": {},
   "source": [
    "plot_model_prediction() 함수를 작성하여, index만 입력해주면 이미지와 실제값, 예측값까지 한 번에 손쉽게 시각화할 수 있도록 하자. 실습 앞부분에서 MNIST 이미지를 시각화하는 함수를 이미 살펴본 바 있으니 참고하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d944bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_prediction(index):\n",
    "    '''index를 입력받아 index에 해당하는 이미지와 예측 결과를 시각화하는 함수'''\n",
    "\n",
    "    true_label_index = label_test[index]\n",
    "    true_label = load_mnist.labels[true_label_index]\n",
    "\n",
    "    with mg.no_autodiff:\n",
    "        # model에 입력해주는 데이터는 4차원이어야 하기 때문에\n",
    "        # 축 하나가 사라지지 않도록 인덱스 설정\n",
    "        prediction = model(x_test[index : index + 1])\n",
    "\n",
    "        # 가장 점수가 높은 라벨이 예측값이 됨\n",
    "        predicted_label_index = np.argmax(prediction.data, axis=1).item()\n",
    "        predicted_label = labels[predicted_label_index]\n",
    "    \n",
    "    \n",
    "    # 이미지, 실제 라벨, 예측 라벨 모두 시각화\n",
    "    \n",
    "    # 여기에 코드 작성\n",
    "    \n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3fed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test의 랜덤 데이터를 시각화\n",
    "index = np.random.randint(0, len(x_test))\n",
    "plot_model_prediction(index);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb9b25a",
   "metadata": {},
   "source": [
    "그다음, 실제 정답 라벨을 사용하여 모델이 잘못 예측한 사례들을 찾고, 그 중 일부를 그래프로 표시해보자. 이때도 plot_model_prediction() 함수를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93739cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측이 틀린 모든 사례 찾아서 해당 인덱스를 bad_indices에 저장\n",
    "bad_indices = []\n",
    "\n",
    "for batch_cnt in range(0, len(x_test) // batch_size):\n",
    "    idxs = np.arange(len(x_test))\n",
    "    batch_indices = idxs[batch_cnt * batch_size : (batch_cnt + 1) * batch_size]\n",
    "    batch = x_test[batch_indices]\n",
    "\n",
    "    with mg.no_autodiff:\n",
    "        # test-batch에 대한 모델의 예측 라벨\n",
    "        prediction = np.argmax(model(batch), axis=1)\n",
    "\n",
    "        # test-batch에 대한 실제 라벨\n",
    "        truth = y_test[batch_indices]\n",
    "        \n",
    "        # 예측과 실제가 다른 경우의 모든 인덱스를 저장\n",
    "        (bad,) = np.where(prediction != truth)\n",
    "        if bad.size:\n",
    "            bad_indices.extend(batch_indices[bad].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0db7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_indices의 랜덤 데이터를 시각화\n",
    "index = np.random.randint(0, len(bad_indices))\n",
    "plot_model_prediction(bad_indices[index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33c5b9",
   "metadata": {},
   "source": [
    "### 배운 내용 되돌아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f429904",
   "metadata": {},
   "source": [
    "이번 실습에서는 0부터 9까지의 손글씨 숫자 이미지를 분류할 수 있는 신경망 모델을 만들기 위해 MNIST 데이터셋을 이용하여 CNN을 학습시켰다. 이 실습에서는 대표적인 구조의 CNN을 구성해보고, 학습시킴으로써 CNN에 대해 잘 이해할 수 있었다. 그리고 그동안 배웠던 신경망 학습의 과정과 유사한 과정이 많아 복습이 많이 되었을 것이다. 배운 내용을 정리해보자.\n",
    "\n",
    "- 유명한 데이터셋 중 하나인 MNIST 데이터셋이 어떻게 구성되어 있는지 탐색해보았다. 특히 shape을 통해 이미지가 몇 개의 색상 채널로 이루어져 있는지 확인하고, 데이터 타입을 통해 각 픽셀에 저장된 값이 uint8임을 확인하였다.\n",
    "\n",
    "- 데이터 전처리를 크게 세 가지 진행하였다. 데이터 타입을 np.float32로 바꾸고, 특성 스케일링(최소-최대 정규화)을 진행하고, 이미지 둘레에 패딩을 도입해주었다.\n",
    "\n",
    "- 패딩을 도입할 때에는 합성곱 연산 이후 원래 이미지의 shape을 유지할 수 있도록하는 폭을 결정해주었다. 패딩의 폭은 하이퍼파라미터로, 합성곱 계층을 정의할 때 지정해주기도 하는 값이다.\n",
    "\n",
    "- 모델(클래스)를 정의할 때에는 가중치의 초기화 방법과 계층의 구성을 결정해야 한다.\n",
    "\n",
    "- 가중치 초기화 방법으로는 자비에 초기화(Xavier Initialization) 중 하나인 glorot_uniform을 사용하였다.\n",
    "\n",
    "- 계층을 구성할 때에는 합성곱-풀링 계층과 밀집층을 각각 두 개씩 총 4층 신경망으로 구성하였으며, 활성함수는 은닉층은 전부 ReLU를, 출력층은 분류문제를 해결하기 위해 Softmax 함수를 사용하였다.\n",
    "\n",
    "- 이번 실습은 실습 6,7과 같은 분류 문제였기 때문에, 정확도 함수는 실습 6에서 살펴보았던 코드와 동일하게 작성해도 충분했다.\n",
    "\n",
    "- Optimizer를 SGD로 선택하여 초기화하는 과정에서 ‘가중치 감소’와 ‘모멘텀’이라는 개념을 배웠다.\n",
    "\n",
    "- 학습을 시키는 과정은 랜덤 batch를 구성하고 Optimizer로 학습시키는 그동안의 실습들과 크게 다르지 않았다. 다만, 데이터셋이 무척 커졌기 때문에 epoch 수를 2로 대폭 줄여 학습을 진행했다.\n",
    "\n",
    "- 이번 실습에서는 학습용 데이터셋에 대한 1 epoch의 학습이 끝날 때마다 테스트용 데이터셋에 대한 정확도를 함께 평가하였다. 처음 접하는 데이터에 대해서도 분류가 잘 이루어지는지 평가하기 위함이었다.\n",
    "\n",
    "- 학습을 진행하면서 noggin 라이브러리를 이용하여 손실과 정확도를 실시간으로 나타내었다. 그리고 1 epoch의 학습이 끝나면 학습용 데이터셋과 테스트용 데이터셋에 대한 정확도를 점으로 찍어 시각화함으로써 비교해볼 수 있었다.\n",
    "\n",
    "- 학습이 모두 끝난 후, 테스트용 데이터셋에 대해 모델의 예측 결과를 시각화하여 관찰하였다. 잘못 분류한 이미지들이 많지 않았기 때문에, 잘못 분류한 이미지만 모아서 시각화해보기도 하였다."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "nbsphinx"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
