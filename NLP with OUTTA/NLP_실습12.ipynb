{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ab84a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mynn.layers.dense import dense\n",
    "from mynn.optimizers.adam import Adam\n",
    "\n",
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "from mygrad.nnet.initializers import glorot_normal\n",
    "from mygrad.nnet.activations import relu\n",
    "\n",
    "import mygrad as mg\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12e8419",
   "metadata": {},
   "source": [
    "<h2>0. Intro</h2>\n",
    "\n",
    "이전 연습문제에서 Seq2Seq 모델을 살펴보았다. 이 모델은 발표된 이후 한동안 학계와 업계에서 지배적으로 활용되고 있었다. Attention 기능까지 더해지면서, Seq2Seq 모델은 대단한 성능과 학습 속도를 보여주었다. 그러나 머지않아 NLP 연구에 새 지평을 연 하나의 새로운 모델이 세상에 공개되었다.\n",
    "\n",
    "그것은 바로 지난 2017년, 구글 브레인이 'Attention is All you Need'라는 제목의 논문으로 공개한 트랜스포머다.\n",
    "\n",
    "논문의 제목에서 알 수 있듯이 트랜스포머는 attention을 기반으로 하며, Seq2Seq과 달리 RNN의 순환 구조가 전혀 사용되지 않았다. 즉, 한 스텝에 하나의 토큰을 처리하는 방식이 아닌, 한 번에 모든 토큰을 처리하는 방식을 취한다. 이로 인해 RNN을 기반으로 한 Seq2Seq보다 훨씬 빠르고, 모델의 복잡도가 현저히 낮다. 이전 스텝과 다음 스텝 간에 유기적인 관계를 맺고 있어 복잡한 양상을 띠는 Seq2Seq와 달리, 트랜스포머는 훨씬 더 간단하고 직관적으로 받아들일 수 있는 행렬곱들로 구성되어 있다.\n",
    "\n",
    "NLP 학계의 연구들을 트랜스포머 이전과 이후로 구분지을 수 있을 만큼, 트랜스포머의 등장은 NLP의 발전에 지대한 영향을 끼쳤다. 2020년 6월 OpenAI에서 선보인 역대 최대 규모의 초거대 언어 AI 모델, GPT-3도 트랜스포머 기반 모델이다. 트랜스포머는 NLP뿐만 아니라 컴퓨터 비전을 비롯한 다른 분야에서도 정말 강력한 도구로서 많이 활용되고 있다.\n",
    "\n",
    "그렇다면 트랜스포머가 도대체 무엇인가? 본 연습문제를 통해 트랜스포머 구조에 대해 깊게 들여다보고자 한다. 트랜스포머 모델을 직접 구현해보고, 이 모델에 암호를 해독하는 방법을 학습시켜보도록 할 것이다.\n",
    "\n",
    "본격적으로 시작하기 전에, 일련의 코드를 통해 줄글 데이터를 불러오도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e720461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63291831 character(s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_to_wikipedia = \"./dat/wikipedia2text-extracted.txt\"\n",
    "with open(path_to_wikipedia, \"rb\") as f:\n",
    "    wikipedia = f.read().decode()\n",
    "    wikipedia = wikipedia.lower()\n",
    "print(str(len(wikipedia)) + \" character(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81bd889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    " \n",
    "# keep alpha + spaces\n",
    "wikipedia = re.sub(\"[^ a-z]\", \"\", wikipedia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c5c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import zip_longest\n",
    " \n",
    "str_len = 25\n",
    "# using all data is like 30 gigs in numpy array form\n",
    "percent_of_data_to_use = 0.05\n",
    " \n",
    "wiki_ls = [''.join(filter(None, s)) for s in zip_longest(*([iter(wikipedia)]*(str_len-1)))]\n",
    "wiki_ls = wiki_ls[:-1] if len(wiki_ls[-1]) != str_len else wiki_ls\n",
    " \n",
    "wiki_ls = wiki_ls[:int(len(wiki_ls) * percent_of_data_to_use)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551e7061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALPHA = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "IND_LOOKUP = {x:i for i, x in enumerate(ALPHA)}\n",
    "LOOKUP_TABLE = np.array([x for x in ALPHA])\n",
    "LOOKUP_TABLE = np.concatenate([np.roll(LOOKUP_TABLE, i)[None] for i in range(0, -len(ALPHA), -1)], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7167aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cipher(plaintext, key):\n",
    "    len_p = len(plaintext)\n",
    "    len_k = len(key)\n",
    "\n",
    "    key = (key * (len_p // len_k + 1))[:len_p]\n",
    "    out = \"\"\n",
    "    for p, k in zip(plaintext, key):\n",
    "        ind_p = IND_LOOKUP[p]\n",
    "        ind_k = IND_LOOKUP[k]\n",
    "\n",
    "        out += LOOKUP_TABLE[ind_k, ind_p]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20007466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key = \"clap\"\n",
    " \n",
    "wiki_enc_ls = [cipher(x, key) for x in wiki_ls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "082eb5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encoding(text):\n",
    "    \"\"\"\n",
    "    adds both a start and end token\n",
    "    \"\"\"\n",
    "    seq_len = len(text)\n",
    "    # +2 to seq_len to add in start/end tokens, +2 to dimension to account for new start/end tokens\n",
    "    # start = [... 0 1 0], end = [... 0 0 1]\n",
    "    out = np.zeros((seq_len+2, len(ALPHA)+2), dtype=np.float32)\n",
    "    inds = [IND_LOOKUP[x] for x in text]\n",
    "    out[range(1, seq_len+1), inds] = 1\n",
    "    out[0, -2] = 1\n",
    "    out[-1, -1] = 1\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db9015be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 125905, 29) (25, 125905, 29)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# src one-hots dont need start token\n",
    "wiki_src_dat = np.concatenate([one_hot_encoding(x)[1:, None] for x in wiki_enc_ls], axis=1)\n",
    " \n",
    "# tgt one-hots dont need end token\n",
    "wiki_tgt_dat = np.concatenate([one_hot_encoding(x)[:-1, None] for x in wiki_ls], axis=1)\n",
    " \n",
    "print(wiki_src_dat.shape, wiki_tgt_dat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac9ac963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 125905)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tgt inds do need end token (and no start)\n",
    "wiki_tgt_inds = np.concatenate([np.array([IND_LOOKUP[x] for x in tgt_str] + [len(ALPHA) + 1]).reshape(-1, 1)\n",
    "for tgt_str in wiki_ls], axis=1)\n",
    " \n",
    "print(wiki_tgt_inds.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "627269c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 2, 29)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dat = [\"cogworks has no asterisk\",\n",
    "\"blahblahblahblahblahblah\"]\n",
    " \n",
    "test_dat = [cipher(x, key) for x in test_dat]\n",
    "test_dat = np.concatenate([one_hot_encoding(x)[1:, None] for x in test_dat], axis=1)\n",
    " \n",
    "print(test_dat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d45c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.savez(\"./dat/wikipedia2text-encoded.npz\",\n",
    "train_src_one_hot=wiki_src_dat,\n",
    "train_tgt_one_hot=wiki_tgt_dat,\n",
    "train_truth_inds=wiki_tgt_inds,\n",
    "test_src_one_hot=test_dat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646329b",
   "metadata": {},
   "source": [
    "<h2>1. 학습 데이터 만들기</h2>\n",
    "\n",
    "본 문제에서는 비즈네르 암호로 인코딩된 글을 데이터셋으로 이용할 것이다. 우리는 입력받은 글에 one-hot 인코딩을 적용하여 2차원 배열로 표현하고, 여기에 비즈네르 암호 체계를 기반으로 한 인코딩 작업을 추가로 진행할 것이다. 이렇게 인코딩된 배열과 이에 대응되는 디코딩된 배열을 이용하여 트랜스포머를 학습시킬 것이다. 이를 통해 임의의 인코딩된 배열을 트랜스포머 모델에 입력했을 때, 인코딩된 문장이 무엇인지 출력할 수 있도록 할 것이다.\n",
    "\n",
    "수열의 각 토큰들은 알파벳 소문자이거나 공백이거나 시작/끝 토큰이다. 당연히 인코딩된 배열에는 시작 토큰이 부여되어 있고, 디코딩된 수열에는 끝 토큰이 부여되어 있다. 그리고 데이터의 차원은 Seq2Seq 연습문제와 동일하다.\n",
    "\n",
    "학습용 데이터와 테스트용 데이터를 추출하고, 각 배열의 모양을 출력해보자. 배열의 각 차원이 말하는 바가 무엇일지 생각해보자.\n",
    "\n",
    "**Note** : 이 문제에서 수열의 길이는 시작 토큰과 끝 토큰을 포함한 길이다. 시작 토큰과 끝 토큰을 제외한 길이를 나타냈던 Seq2Seq 연습문제와는 다르다는 점에 유의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47cd2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with np.load(\"./dat/wikipedia2text-encoded.npz\") as data:\n",
    "    train_src_one_hot = data[\"train_src_one_hot\"]\n",
    "    train_tgt_one_hot = data[\"train_tgt_one_hot\"]\n",
    "    train_truth_inds = data[\"train_truth_inds\"]\n",
    "    test_src_one_hot = data[\"test_src_one_hot\"]\n",
    "\n",
    "# 여기에 코드 작성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae34fc6e",
   "metadata": {},
   "source": [
    "<h2>2. 트랜스포머 모델의 구조</h2>\n",
    "\n",
    "자, 본격적으로 연습 문제를 시작하기에 앞서, 트랜스포머의 구조를 살펴보자. 각 층이 어떤 기능을 하는지 전혀 감이 안 잡힐 수도 있겠지만, 큰 그림을 그리는데 도움이 많이 될 것이라 생각한다. 트랜스포머의 구조는 책에 수록된 그림 3.6-7과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c218f46",
   "metadata": {},
   "source": [
    "트랜스포머는 크게 인코더와 디코더, 두 가지로 나눌 수 있다. 인코더의 경우, Multihead Attention 기반 층과 Feed Forward 기반 층으로 구성된 층이 N번 반복된다. 그리고 디코더의 경우, Multihead Attention 기반 층 2개와 Feed Forward 기반 층으로 구성된 층이 N번 반복된다.\n",
    "\n",
    "입력 값과 인코더, 입력 값과 디코더 사이를 보면, Positional Encoding이라는 것이 적용된다. 후술하겠지만, 이는 입력된 각 값들의 위치 정보를 함의한다. 이렇게 각 입력 값에 위치 정보를 넣어줌으로써 RNN과 같은 순환적 구조가 아님에도 입력 값들의 위치를 트랜스포머가 인지할 수 있는 것이다.\n",
    "\n",
    "Multihead Attention이 어떤 것인지에 대해서는 곧 서술하겠으나, 인코더의 Multihead Attention과 달리 디코더의 Multihead Attention에는 마스킹 기법이 쓰이는 것으로 보인다. 이는 디코더가 얻을 수 있는 정보를 제한하기 위함이다.\n",
    "\n",
    "예를 들어, I want to play soccer이라는 문장을 한국어로 번역하고 있다고 생각해보자.  이것을 번역하게 된다면, ‘나는 축구를 하고 싶어’라는 문장으로 번역될 수 있을 것이다. 여기서, ‘나는 축구를’까지 해석이 되었다고 생각해보자. 그렇다면 이어서 ‘하고’라는 단어를 예측하는데 있어서 ‘나는 축구를’이 출력되었다는 정보만 다루어야 한다. 그 뒤에 나올 단어들에 대해서는 아는 정보가 없어야 한다. 상식적으로 생각해보면, 어떤 단어가 나오면 그것을 토대로 다음 단어를 예측하는 것이 골자이기에, 다음 단어를 가지고 이전 단어를 예측하는 것은 이치에 다소 어긋나는 것이다. 그렇기에 마스킹 기법을 활용하여, 뒤 토큰들의 예측 상황을 제하는 것이다. \n",
    "\n",
    "그리고 모든 Multihead Attention과 Feed Forward 층 다음에는 Add & Norm이 있다. 이것은 앞서 ‘신경망 성능 향상을 위한 기법들’에서 다룬 Residual Connection과 Layer Normalization 기법을 의미한다. Residual Connection은 결과 값에 입력 값을 더해주는 것이므로 Add, Layer Normalization은 Norm으로 표현되었다.\n",
    "\n",
    "구조를 한 번 훑었으니, 각 층이 구체적으로 어떤 기능을 하는지 알아보도록 하자. 특히 Multihead Attention이라는 용어가 많이 등장했는데, 이 기법이 무엇인지 한 번 자세히 들여다보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95aa88d",
   "metadata": {},
   "source": [
    "<h2>3. Scaled Dot Product Attention</h2>\n",
    "\n",
    "Multihead Attention을 알아보기에 앞서, 그것의 근간을 이루는 Scaled Dot Product Attention에 대해 살펴보도록 하자. 이름에서 볼 수 있듯이 내적을 활용하며, 내적을 통해 Attention Score를 계산하는 기법이다. (* dot product = 내적)\n",
    "\n",
    "이를 자세히 들여다보기 전에, Seq2Seq With Attention 연습문제에서 배웠던 Attention에 대해 다시 한 번 상기시켜보자. \n",
    "\n",
    "attention 기법은 직전 디코더 hidden state와 인코더 hidden state들간의 유사도를 각각 구하여, 다음 디코더 hidden state를 생성할 때 여러 인코더 hidden state들 중 어떤 것을 중시해야 하는지 판단하는 것이다. Attention 기법을 활용하기 위해서는 인코더 hidden state들과 직전 디코더 hidden state들 사이의 관계를 나타내는 Attention Weight를 구해야 했다. 그리고 이 배열을 이용해 인코더 hidden state들의 가중합으로써 다음 디코더 hidden state를 산출했다.\n",
    "\n",
    "Scaled Dot Product Attention도 위와 유사한 방식으로 작동한다. 정확히 어떤 방식으로 구성되어 있는지 아래에서 살펴보도록 하자.\n",
    "\n",
    "Scaled Dot Product Attention은 이 세 가지를 입력받는다 : query Q, key K, value V\n",
    "\n",
    "위 Q, K, V가 지칭하는 값은 아래와 같다.\n",
    "\n",
    "- Query : 직전 디코더의 hidden state\n",
    "- Key : 인코더의 전체 hidden state\n",
    "- Value : 인코더의 전체 hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b4c05",
   "metadata": {},
   "source": [
    "여기서 K와 V는 같은 값을 갖는데, 각 벡터의 의미가 조금 다르다.\n",
    "\n",
    "key의 경우, query와의 연관성 측정을 위해 즉, 직전 디코더의 hidden state와 인코더의 hidden state 사이의 관계를 파악하는 데 쓰인다. 이렇게 측정된 연관성은 Seq2Seq에서의 attention weight을 나타낸다. 그리고 value의 경우, query와 key를 통해 계산된 attention weight으로 가중합을 산출하여 궁극적으로 디코더의 새로운 hidden state를 계산해내는데 쓰인다.\n",
    "\n",
    "어떻게 하면 attention score를 구할 수 있을까? attention score의 경우, query와 key의 내적을 통해 구할 수 있다. 그리고 여기에 softmax 함수를 적용하여 attention weight를 구한다.\n",
    "\n",
    "이렇게 계산된 attention weight은 다음 디코더 hidden state를 계산하는 데 있어서 어떤 인코더 hidden state가 중요하게 여겨져야 할지 표현해준다. 위와 같은 연산 과정이 어떻게 이루어지는 것인지 수식을 통해 좀 더 명확히 살펴볼 수 있도록 하자.\n",
    "\n",
    "Q는 (t, d) 모양의 배열일 것이고, K와 V는 (T, d) 모양의 배열일 것이다. 셋 모두, 각 행은 토큰을 one-hot 인코딩한 배열을 나타낸다. 예를 들어, Q의 경우 아래와 같을 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb0fa1b",
   "metadata": {},
   "source": [
    "$$Q=\\begin{bmatrix}\\overrightarrow{q_1}\\\\ \\overrightarrow{q_2} \\\\ ⦙ \\\\ \\overrightarrow{q_t}\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56103420",
   "metadata": {},
   "source": [
    "**Note** : 여기서 각 배열의 길이를 나타내는 T와 t의 경우, 같은 값을 가져도 되고, 다른 값을 가져도 된다.\n",
    "\n",
    "이제, Q와 K를 이용하여 attention score를 계산해보자. Q와 K의 전치 행렬에 행렬곱을 진행해서 각 query 벡터와 key 벡터의 내적을 구해볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5565f1b",
   "metadata": {},
   "source": [
    "$$E=QK^T=\\begin{bmatrix}\\overrightarrow{q_1}\\\\ \\overrightarrow{q_2}\\\\ ⦙ \\\\ \\overrightarrow{q_t}\\end{bmatrix}\\begin{bmatrix}\\overrightarrow{k_1} & \\overrightarrow{k_2} & ... & \\overrightarrow{k_T}\\end{bmatrix}$$\n",
    "\n",
    "$$=\\begin{bmatrix}\\overrightarrow{q_1}*\\overrightarrow{k_1} & \\overrightarrow{q_1}*\\overrightarrow{k_2} & ... & \\overrightarrow{q_1}*\\overrightarrow{k_T} \\\\ \\overrightarrow{q_2}*\\overrightarrow{k_1} & \\overrightarrow{q_2}*\\overrightarrow{k_2} & ... & \\overrightarrow{q_2}*\\overrightarrow{k_T} \\\\ ⦙ & ⦙ & & ⦙ \\\\ \\overrightarrow{q_t}*\\overrightarrow{k_1} & \\overrightarrow{q_t}*\\overrightarrow{k_2} & ... & \\overrightarrow{q_t}*\\overrightarrow{k_T}\\end{bmatrix}=\\begin{bmatrix}e_{1,1} & e_{1,2} & ... & e_{1,T} \\\\ e_{2,1} & e_{2,2} & ... & e_{2,T} \\\\ ⦙ & ⦙ & & ⦙ \\\\ e_{t,1} & e_{t,2} & ... & e_{t,T}\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b7f46",
   "metadata": {},
   "source": [
    "이로써, (t, T) 모양의 배열 E가 생성되었다. 여기서 $\\overrightarrow{e_{i,j}}$는 i번째 query 벡터와 j번째 key 벡터 사이의 attention score를 말한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa4967",
   "metadata": {},
   "source": [
    "이제 이 점수에 softmax 함수를 적용하여 attention weight를 계산하기 이전에, 우리는 attention score를 $\\sqrt{d_k}$로 나눌 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f71f637",
   "metadata": {},
   "source": [
    "$$E'=1/\\sqrt{d_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18ec72",
   "metadata": {},
   "source": [
    "**Note** : 이는 softmax 함수를 적용하는 데 있어 attention score의 범위를 보다 더 합리적으로 설정해준다. 절댓값이 큰 음수의 경우 softmax 함수를 적용했을 때, 0에 매우 근접한 값을 가지게 되어 훈련의 효과가 감소하게 된다. 고로 $\\sqrt{d_k}$로 나눠 줌으로써 위와 같은 vanish 현상의 빈도를 줄여서 학습 효율을 높여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1307df69",
   "metadata": {},
   "source": [
    "이제 attention weight를 구하기 위해 $E'$에 softmax 함수를 적용해볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e7c15",
   "metadata": {},
   "source": [
    "$$a=softmax(E')=\\begin{bmatrix}softmax(e'_{1,1} & e'_{1,2} & ... & e'_{1,T}) \\\\ softmax(e'_{2,1} & e'_{2,2} & ... & e'_{2,T}) \\\\ & ⦙ & \\\\ softmax(e'_{t,1} & e'_{t,2} & ... & e'_{t,T})\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609e711",
   "metadata": {},
   "source": [
    "Seq2Seq에서 했던 것처럼, 우리는 이 attention weight를 이용해 value 벡터의 가중합을 구할 것이다. 아래와 같이 진행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6a1df7",
   "metadata": {},
   "source": [
    "$$C=aV=\\begin{bmatrix}a_{1,1} & a_{1,2} & ... & a_{1,T} \\\\ a_{2,1} & a_{2,2} & ... & a_{2,T} \\\\ ⦙ & ⦙ & & ⦙ \\\\ a_{t,1} & a_{t,2} & ... & a_{t,T}\\end{bmatrix}\\begin{bmatrix}\\overrightarrow{v_1}\\\\ \\overrightarrow{v_2} \\\\ ⦙ \\\\ \\overrightarrow{v_T}\\end{bmatrix}$$\n",
    "\n",
    "$$=\\begin{bmatrix} \\sum_{i=1}^Ta_{1,i}(\\overrightarrow{v_i})_1 & \\sum_{i=1}^Ta_{1,i}(\\overrightarrow{v_i})_2 & ... & \\sum_{i=1}^Ta_{1,i}(\\overrightarrow{v_i})_d \\\\ \\sum_{i=1}^Ta_{2,i}(\\overrightarrow{v_i})_1 & \\sum_{i=1}^Ta_{2,i}(\\overrightarrow{v_i})_2 & ... & \\sum_{i=1}^Ta_{2,i}(\\overrightarrow{v_i})_d \\\\ ⦙ & ⦙ & & ⦙ \\\\ \\sum_{i=1}^Ta_{t,i}(\\overrightarrow{v_i})_1 & \\sum_{i=1}^Ta_{t,i}(\\overrightarrow{v_i})_2 & ... & \\sum_{i=1}^Ta_{t,i}(\\overrightarrow{v_i})_d \\end{bmatrix}=\\begin{bmatrix}\\overrightarrow{c_1}\\\\\\overrightarrow{c_2}\\\\⦙\\\\\\overrightarrow{c_t}\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9a9d6",
   "metadata": {},
   "source": [
    "위와 같이 배열 C를 계산할 수 있고, 여기서 각 행은 Seq2Seq에서의 문맥 벡터와 같은 의미를 지닌다고 생각해도 좋다. 그리고 우리는 K와 V를 같은 값을 갖는다고 가정할 것이다. \n",
    "만일 이 모델을 Seq2Seq 모델의 연장선 상에서 본다면, K와 V 둘 다 $\\overrightarrow{H^e}$로 볼 수 있고, Q는 $\\overrightarrow{h_t^d}$로 볼 수 있다.\n",
    "\n",
    "\n",
    "Scaled Dot Product Attention을 정리하자면 아래와 같이 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039520ee",
   "metadata": {},
   "source": [
    "$$C=Attention(Q,K,V)=softmax(QK^T/\\sqrt{d_k})V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49cc403",
   "metadata": {},
   "source": [
    "<h2>4. Multihead Attention</h2>\n",
    "\n",
    "Scaled Dot Product Attention에 대해 살펴보았으니, Multihead Attention에 대해 본격적으로 살펴보자.\n",
    "\n",
    "Scaled Dot Product Attention은 연산 과정이 직렬적으로 이어져 있어 오랜 시간을 들여야 한다는 단점이 있다. 그러나 Multihead Attention의 경우 연산 과정을 '병렬화', 즉 여러 개로 쪼개어 각각을 동시에 실행하므로 시간이 훨씬 절약된다. 우리는 Multihead Attention 기법을 통해 하나의 Attention 연산을 h개의 attention 연산으로 쪼개서 동시에 h개의 연산을 진행할 수 있도록 할 것이다. 이렇게 되면, 한 번의 큰 연산을 진행할 때보다 h배 가량 빠른 연산이 가능할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669f1a5",
   "metadata": {},
   "source": [
    "<h3>Step 1. attention head 나누기</h3>\n",
    "\n",
    "우리는 나눠진 attention 연산 각각을 attention head라고 부를 것이다. 이렇게 여러 attention head를 동시다발적으로 진행함에 따라 더 많은 양의 데이터를 빠른 시간 안에 학습할 수 있게 되었다.\n",
    "\n",
    "이에 대한 설명에 앞서, 학습 가능한 가중치 배열들 $(W_q^{(i)})_{i=1}^h$, $(W_k^{(i)})_{i=1}^h$, $(W_v^{(i)})_{i=1}^h$를 정의할 것이다. 이 가중치 배열들은 하나의 큰 attention head를 h개의 attention head로 분산시키는데 사용된다. 어떤 방식으로 이루어지는 것일까?\n",
    "\n",
    "기존 하나의 큰 attention head의 경우, Q, K, V 벡터를 입력받아서 그대로 Scaled Dot Product Attention을 수행해준다. 이에 반해 Multihead Attention은 Q, K, V를 입력받고 각각에 대응되는 h개의 가중치 배열을 곱해준다. 그렇게 진행해준다면, Q, K, V가 각각 h개씩 새로 생성될 것이다. 그렇게 생성한 h개의 Q, K, V를 가지고 h번의 Scaled Dot Product Attention을 수행한다. 이로써 h개의 attention head가 구성되는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c44df",
   "metadata": {},
   "source": [
    "각각의 이름에서 알 수 있듯이, 위의 가중치 배열들은 각각 차원의 Q, K, V를 $d_k$나 $d_v$차원 공간에 정사영시키는데 사용될 것이다. 각 벡터마다 h개의 가중치 배열들을 통해 h가지의 서로 다른 방식으로 투영되기 때문에 모델은 query, key, value 벡터의 다양한 패턴을 인지하고 학습할 수 있다. 여기서 h는 attention head의 개수를 말하며, 배열 $W_q$, $W_k$, $W_v$은 각각 $(d,d_k)$, $(d,d_k)$, $(d,d_v)$의 모양을 지닌다.\n",
    "여기서 $d_k$, $d_v$값의 경우, 우리 임의대로 정할 수 있으므로 아래와 같이 정의할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcaef6e",
   "metadata": {},
   "source": [
    "이렇게 한 개의 거대한 attention head를 h개의 작은 attention head들로 나눠서 연산하는 multihead attention의 도입 덕분에, 많은 수의 attention head들을 수행하는데 필요한 computational cost가 마치 적은 수의 attention head들을 수행하는 것과 비슷하게 된다.\n",
    "\n",
    "이제 각 attention head에 입력되어야 하는 query, key, value 벡터를 직접 산출해보자. 각각은 query, key, value 벡터를 각 배열에 대응되는 가중치 배열과 곱함으로써 구해진다. i번째 attention head가 입력으로 받게 될 query, key, value 벡터는 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa60ec4",
   "metadata": {},
   "source": [
    "$$Q^{(i)}=QW_q^{(i)}\\rightarrow(t,d)*(d,d_k)\\rightarrow(t,d_k)$$\n",
    "\n",
    "$$K^{(i)}=KW_k^{(i)}\\rightarrow(T,d)*(d,d_k)\\rightarrow(T,d_k)$$\n",
    "\n",
    "$$V^{(i)}=VW_v^{(i)}\\rightarrow(T,d)*(d,d_v)\\rightarrow(T,d_v)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0610c",
   "metadata": {},
   "source": [
    "위에서 언급한 바 Q, K, V의 각 벡터마다 곱해지는 가중치 배열이 서로 다르다. 이에 따라 각 벡터가 서로 다른 정보를 함의한 채 각기 다른 방식으로 모델에 투영되므로. Q, K, V 벡터들이 d_k나 d_v 차원의 벡터 h개 그 이상의 의미를 지니게 된다. 결과적으로 모델은 다양한 패턴을 인지하고 학습하게 되는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549d363",
   "metadata": {},
   "source": [
    "<h3>Step 2. attention weight 계산하기</h3>\n",
    "\n",
    "이제 우리는 h개의 ($Q^{(i)}$, $K^{(i)}$, $V^{(i)}$)에 scaled dot product attention을 적용하여 $(t, d_v)$ 모양의 배열을 h개 출력할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c656b",
   "metadata": {},
   "source": [
    "$$C^{(i)}=Attention(Q^{(i)}, K^{(i)}, V^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ba559",
   "metadata": {},
   "source": [
    "하지만, 우리는 attention 메커니즘을 통해 하나의 배열만을 출력해야 하기 때문에 h개의 attention head 출력 값을 결합할 것이다. 우리는 아래와 같이 열들을 이어붙임으로써 attention head들을 결합시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f190ea7",
   "metadata": {},
   "source": [
    "$$C=\\begin{bmatrix}\\overrightarrow{c_1}^{(1)} & \\overrightarrow{c_1}^{(2)} & ... & \\overrightarrow{c_1}^{(h)} \\\\ \\overrightarrow{c_2}^{(1)} & \\overrightarrow{c_2}^{(2)} & ... & \\overrightarrow{c_2}^{(h)} \\\\ ⦙ & ⦙ & & ⦙ \\\\ \\overrightarrow{c_t}^{(1)} & \\overrightarrow{c_t}^{(2)} & ... & \\overrightarrow{c_t}^{(h)}\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c0974",
   "metadata": {},
   "source": [
    "이제 $(t,h*\\lfloor d/h \\rfloor)$ 모양의 배열 C를 얻었다. 이때 우리가 얻고자 하는 최종 출력의 모양은, 연산 과정을 여러 개로 나누기 전 즉 직렬적으로 연산을 수행했을 때와 동일한 형태여야 한다. 즉, $(t,d)$의 모양을 얻고자 한다. 이를 위해 우리는 모양의 가중치 배열 $W_o$를 곱할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816bb33",
   "metadata": {},
   "source": [
    "$$C_{out}=CW_o$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ddbf9",
   "metadata": {},
   "source": [
    "$W_o$는 attention head에서 연산된 결과에서 중요한 정보들을 요약해 출력해줄 것이다. 결론적으로, 최종 결과 값 $C_{out}$은 $(t,d)$ 모양의 배열로 query 벡터와 같은 모양을 띨 것이다.\n",
    "\n",
    "이제 우리는 여러 개의 attention head를 동시에 수행하는 것뿐만 아니라 데이터셋을 여러 개의 batch로 나눠, 여러 개의 batch에 대해서도 동시에 학습을 진행할 것이다. 어떻게 하면 여러 batch들에 대해서 동시에 행렬곱 연산을 진행할 수 있는 것인지 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d4f9cd",
   "metadata": {},
   "source": [
    "<h2>4-1. Batch를 기준으로 한 병렬화</h2>\n",
    "\n",
    "multihead attention에 입력될 batch는 길이 t 혹은 T의 배열이 N개 들어있는 batch라고 가정해보자. 배열의 각 토큰은 d 차원의 벡터로 표현될 것이며, query, key, value 벡터의 모양은 아래와 같다.\n",
    "\n",
    "Q : (t, N, d), K : (T, N, d), V : (T, N, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef915df",
   "metadata": {},
   "source": [
    "<h3>Step 1. Attention head 나누기</h3>\n",
    "\n",
    "multihead attention을 실행하기 위해서는 위에서 언급한 바, 학습 가능한 가중치 배열들도 정의해주어야 한다. $W_q$, $W_k$, $W_v$는 각각 $(d,d_k)$, $(d,d_k)$, $(d,d_v)$의 모양을 가질 것이다. 그리고 위에서 언급한 바와 같이, $d_k=d_v=\\lfloor d/h \\rfloor$로 값을 설정해줄 것이다. 그럴 경우, 가중치 배열들의 모양은 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0baec0",
   "metadata": {},
   "source": [
    "<center>Wq : (h, d, d // h) = (h, d, k)\n",
    "<center>Wk : (h, d, d // h) = (h, d, k)\n",
    "<center>Wv : (h, d, d // h) = (h, d, v) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e837067",
   "metadata": {},
   "source": [
    "마지막으로 h개의 문맥 벡터들을 이어붙인 $(t,h*\\lfloor d/h \\rfloor)$차원의 배열을 $(t,d)$차원 배열로 만들어주는데 필요한 $W_o$를 정의해주어야 한다. 이 배열의 모양은 아래와 같다.\n",
    "\n",
    "<center>Wo : (h * (d // h), d) = (h * v, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba6cc8",
   "metadata": {},
   "source": [
    "이로써 필요한 변수들을 모두 정의했다. 이제 어떻게 연산을 수행할 것인지 살펴보도록 하자.\n",
    "\n",
    "multihead attention에서 가장 먼저 해주어야 할 연산은 query, key, value 벡터를 각각에 대응되는 가중치 배열과 곱해주는 것이다. 그래야 하나의 거대한 attention 연산을 h개의 attention head로 나눌 수 있기 때문이다.\n",
    "\n",
    "h개의 attention head의 연산을 구분하기 위해, 행렬곱을 수행할 때 가중치 배열에 h-차원 브로드캐스팅을 해주어야 한다. 비슷한 논리로 batch에 있는 각 수열들에 대해 동시에 attention weight를 계산하기 위해, N-차원 브로드캐스팅을 해줄 것이다. 고로, 각각의 행렬 곱이 이루어진 값의 모양은 아래와 같을 것이다.\n",
    "\n",
    "Q_i = Q x Wq : (t, N, d) x (h, d, k) -> (t, N, h, k)\n",
    "K_i = K x Wk : (T, N, d) x (h, d, k) -> (T, N, h, k)\n",
    "V_i = V x Wv : (T, N, d) x (h, d, v) -> (T, N, h, v)\n",
    "\n",
    "d-차원에 대해 행렬 곱이 진행되어 Q_i, K_i, V_i에는 d-차원이 존재하지 않는다.\n",
    "\n",
    "**Note** : 이어질 두 가지의 연산을 파헤쳐보는데 있어 N과 h는 무시하는 것이 도움이 될 수 있다. 위의 식처럼 행렬 곱이 진행될 때, batch와 attention head의 차원에 대한 브로드캐스팅이 일어나서, 실질적으로 행렬 곱이 발생하는 곳은 첫 번째 축과 마지막 축(T or t, k or v)이기 때문이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38b792",
   "metadata": {},
   "source": [
    "<h3>Step 2. attention weight 계산하기</h3>\n",
    "\n",
    "이제 우리는 각각의 attention head를 수행하여 attention weight를 계산해낼 것이다. attention weight를 계산하기 위해서는 우선 query 벡터와 key 벡터의 유사성을 나타내는 query 벡터와 key 벡터의 내적을 계산해주어야 한다. 이전 연산들과 마찬가지로, Q_i, K_i 모두에게 batch와 attention head의 차원을 기반으로 한 브로드캐스팅이 이루어진다. 각 행렬들의 모양은 아래와 같을 것이다.\n",
    "\n",
    "E = Q_i x K_i : (t, N, h, k) x (T, N, h, k) -> (t, N, h, T)\n",
    "\n",
    "k-차원 벡터에 대하여 행렬 곱 연산이 이루어져서, E에는 k-차원이 존재하지 않는다.\n",
    "이로써 attention score E를 구했다. 이제 위에서 구한 E를 $\\sqrt{d_k}$로 나누어주고, T-차원에 대해 softmax를 적용해준 후, attention weight을 구하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc306295",
   "metadata": {},
   "source": [
    "attention weight들과 V_i들을 곱해줌으로써 각 attention head 별 value 벡터의 weighted sum을 구할 것이다. 아래와 같이 T-차원에 대해 행렬 곱 연산을 진행하고 결과 값 C를 출력한다.\n",
    "\n",
    "C_i = α x V_i : (t, N, h, T) x (T, N, h, v) -> (t, N, h, v)\n",
    "\n",
    "이로써, 각 attention head별 문맥 벡터를 계산하는 것까지 진행했다. C_i는 한 batch에 담긴 N개의 수열에 대한 v-차원의 문맥 벡터를 t개 출력하는 attention head h개의 연산 결과다. 그러나 우리는 attention head 여러 개로 분리된 결과가 아니라 attention head가 하나일 때의 결과값과 같은 모양의 값을 얻고 싶다. 그렇기에 attention head를 기준으로 분리된 축들을 하나로 병합할 것이다.\n",
    "\n",
    "C_i -> C : (t, N, h, v) -> (t, N, h * v)\n",
    "\n",
    "그리고 우리는 (t, N, d) 모양의 최종 결과 값을 얻고자 하므로, 배열 C에 (h * v, d) 모양의 배열 Wo을 추가로 곱하여 원하는 모양의 행렬을 출력받을 것이다.\n",
    "\n",
    "C_out = C x Wo : (t, N, h * v) x (h * v, d) -> (t, N, d)\n",
    "\n",
    "이렇게 C_out 배열를 계산함으로써 Multihead Attention에 관련한 모든 연산을 마쳤다.\n",
    "\n",
    "이제 위 연산을 코드로 직접 구현해보도록 하자. 아래에 코멘트들을 달아서 다양한 행렬 곱을 구현하는데 필요한 지시사항들을 제공했으니 잘 반영할 수 있도록 하자. 그리고 MyGrad 라이브러리의 einsum 함수 역시 많은 부분에서 필요하니 적극적으로 활용하도록 하자. 자, 그럼 코드로 넘어가보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7c77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiheadAttention:\n",
    "    def __init__(self, dim, n_head=3):\n",
    "        \"\"\" multihead attention을 구현해보자.\n",
    "\n",
    "        매개변수(Parameters)\n",
    "        ----------\n",
    "        dim : int\n",
    "        입력 값의 차원을 말한다.\n",
    "        one-hot 인코딩 관점에서 본다면, 단어의 개수다.\n",
    "        n_head : int\n",
    "        attention head의 개수를 말한다.\n",
    "        \"\"\"\n",
    "        # self.Wq를 glorot-normal 분포로부터 받아오도록 하자.\n",
    "        # 배열은 3차원이며, (h, d, d // h)의 모양을 가질 것이다.\n",
    "        # 여기에 코드 작성\n",
    "\n",
    "        # self.Wk를 glorot-normal 분포로부터 받아오도록 하자.\n",
    "        # 배열은 3차원이며, (h, d, d // h)의 모양을 가질 것이다.\n",
    "        # 여기에 코드 작성\n",
    "\n",
    "        # self.Wv를 glorot-normal 분포로부터 받아오도록 하자.\n",
    "        # 배열은 3차원이며, (h, d, d // h)의 모양을 가질 것이다.\n",
    "        # 여기에 코드 작성\n",
    "\n",
    "        # self.Wo를 MyNN dense class로 설정해줄 것이다.\n",
    "        # 이는 (t, N, h * [d // h]) 모양의 배열을 (t, N, d) 모양의 배열로 바꿔준다\n",
    "        # 이 밀집층은 편향 값을 가지지 않으며,\n",
    "        # 가중치 값은 glorot_normal 분포로부터 받아올 것이다.\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def __call__(self, Q, K, V, mask=None):\n",
    "        \"\"\" attention을 이용한 순전파를 구현해보자.\n",
    "\n",
    "        매개변수(Parameters)\n",
    "        ----------\n",
    "        Q : Union[numpy.ndarray, mygrad.Tensor], shape=(t, N, d)\n",
    "        query 벡터. 이 벡터는 어텐션 점수를 계산해가면서 차츰 바뀔 것이다.\n",
    "\n",
    "        K : Union[numpy.ndarray, mygrad.Tensor], shape=(T, N, d)\n",
    "        key 벡터. 어텐션 점수를 계산하는 데 있어 이 벡터에 담긴 정보들을\n",
    "        끌어온다. key 벡터의 토큰 각각은 query 벡터의 토큰들과 비교되고,\n",
    "        key와 query의 유사성 정도에 따라 어텐션 점수가 결정된다.\n",
    "\n",
    "        V : Union[numpy.ndarray, mygrad.Tensor], shape=(T, N, d)\n",
    "        value 벡터. 이 벡터와 어텐션 점수를 기반으로 가중합이 구해진다. 이 벡터 는 종종 x_k라고 불릴 것이다.\n",
    "\n",
    "        mask : Optional[numpy.ndarray], dtype=bool, shape=(t, T)\n",
    "        어텐션 가중치에 적용하는 마스크.\n",
    "        False로 설정된 값들을 softmax가 적용되기 이전에 -1e14로 바꾸어 준다.\n",
    "\n",
    "        반환 값(Returns)\n",
    "        -------\n",
    "        mygrad.Tensor\n",
    "        value 벡터 V의 가중합이다. 가중치들은 query 벡터와 key 벡터의 유사성에\n",
    "        기반하여 설정되었다.\n",
    "        \"\"\"\n",
    "        # ‘self.Wq', 'self.Wk', 'self.Wv'를 이용하여 Q_i, K_i, V_i를 계산해보자.\n",
    "        # 각각은 4차원 벡터여야 하며, (t or T, N, h, d // h)의 모양을 지녀야 한다.\n",
    "        #\n",
    "        # 위 텐서는 batch의 각 수열 그리고 각 attention head에 대해\n",
    "        # Q_i, K_i, V_i를 포함해야 한다.\n",
    "        # 여기에 코드 작성\n",
    "\n",
    "        # 아래에서 우리는 scaled dot product attention을 이용하여\n",
    "        # 투영된 query, key, value 벡터를 구해볼 것이다.\n",
    "        #\n",
    "        # Q_i와 K_i의 전치행렬(첫 번째 축과 마지막 축만 교환한다)을 곱하고,\n",
    "        # 'E'에 결과를 저장하자. (t, N, h, T)의 모양을 지녀야 한다.\n",
    "        # 행렬곱 과정에서 batch와 attention head 사이에\n",
    "        # 브로드캐스팅이 반드시 발생하도록 해야 할 것이다.\n",
    "        #\n",
    "        # 그리고, ‘E'를 sqrt(d_k)로 나누고 그 결과를 'E'에 다시 저장한다.\n",
    "        # 여기에 코드 작성\n",
    "\n",
    "        # 이제 위에 언급한 마스크를 사용해볼 것이다. 마스크는 (t, T) 모양의 배열이며,\n",
    "        # False 값을 나타내는 곳이 있다면, 그 위치에 해당하는 어텐션 가중치 값을\n",
    "        # 0으로 설정한다. 마스크는 batch와 attention head의 차원에 맞춰서\n",
    "        # 브로드캐스팅하게 될 것이다.\n",
    "\n",
    "        if mask is not None:\n",
    "            E.transpose(1, 2, 0, 3)[:, :, ~mask.astype(np.bool_)] = -1e14\n",
    "        # 마스킹된 점수의 마지막 축을 따라 softmax를 적용하고,\n",
    "        # 어텐션 가중치를 `self.a_ij`에 저장해놓자.\n",
    "        # 여기에 코드 작성\n",
    "\n",
    "        # 어텐션 가중치와 V_i를 곱해주고, (T, N, h, d // h)의 모양을 갖기 위해\n",
    "        # 행렬 곱 결과의 전치 행렬을 출력해주자.\n",
    "        # 여기에 코드 작성\n",
    "\n",
    "        # 이제 우리는 각 attention head의 연산을 진행했고, 결과 값들을 합쳐서\n",
    "        # 하나의 배열로 출력할 것이다.\n",
    "        #\n",
    "        # (t, N, h, d // h) 모양의 텐서를 (t, N, h * [d // h]) 모양의 텐서로 바꾸자.\n",
    "        # 이는 attention head의 연산 결과 값들을 열로 간주하고 이어붙이는 것과\n",
    "        # 동등한 작업이다.\n",
    "        #\n",
    "        # 이제 마지막 밀집 층인 Wo를 적용하고, (t, N, d) 모양의 텐서를 출력하자.\n",
    "        # 여기에 코드 작성\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" 모델의 모든 파라미터를 리턴해주는 편리한 함수 \"\"\"\n",
    "        # 여기에 코드 작성\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32bc990",
   "metadata": {},
   "source": [
    "<h2>5. 인코더</h2>\n",
    "\n",
    "Seq2Seq 모델처럼 트랜스포머 또한 인코더와 디코더를 갖고 있다. 인코더는 self-attention를 이용하여 주어진 글 안의 중요한 관계와 패턴을 추출해낸다. Self-attention은 무엇인가? 잠시 이에 대해 살펴보자.\n",
    "\n",
    "우리는 위에서 Q, K, V 벡터를 입력받아서 Attention을 구하는 Multihead Attention 메커니즘에 대해 알아보았다. Attention은 Q, K, V 벡터를 입력받아서 행렬 곱 연산을 통해 유사도를 구함으로써 전체 입력 벡터 가운데 다음 벡터 예측을 위해 가장 집중해야 하는 입력 벡터가 무엇인지 판단하는 기법이다.\n",
    "\n",
    "Self-Attention은 Q, K, V 벡터 모두에 같은 값을 넣어서 Attention을 계산하는 기법을 의미하는데, 입력받은 벡터 자체로 Attention을 계산함으로써 해당 벡터 내 토큰들 사이의 연관성을 알려준다. 즉, 입력받은 벡터의 토큰들 각각이 벡터 내의 어떤 토큰들과 얼마나 관련이 있는지 찾아내어 입력 벡터 내 단어들 사이의 관계를 파악하는 것이다.\n",
    "\n",
    "그렇다면 인코더는 어떤 구조를 가지고 있는 것일까? 이에 대해 살펴보기 전에, 앞서 다뤘던 신경망의 성능을 향상시켜주는 기법들에 대한 기억을 되살려보자.\n",
    "\n",
    "Batch Normalization와 Layer Normalization의 경우 모든 층마다 일종의 표준화 과정을 도입함으로써 신경망의 성능을 향상시킬 수 있으며,  Residual Connection를 추가하는 경우 결과 값에 입력 값을 더해주어 gradient vanish 현상을 방지할 수 있었다. 이들은 트랜스포머의 인코더와 디코더에 모두 활용되며, 혹여나 내용이 헷갈린다면, 해당 본문으로 돌아가서 다시 살펴보고 오도록 하자.\n",
    "\n",
    "Batch Normalization의 경우, 데이터의 차원에 제한점이 있어 NLP에 적용하기 다소 어려운 점이 있었다. 이에 따라 본 트랜스포머 모델의 인코더와 디코더에는 Layer Normalization과 Residual Connection을 활용할 것이다. 모든 층이 끝날 때마다 Layer Norm과 Residual Connection을 적용해줄 것이다.\n",
    "\n",
    "아래의 ResidualConnectAndLayerNorm은 Layer Normalization과 Residual Connection을 수행하는 역할을 하는 클래스다. 이 클래스는 트랜스포머의 각 층의 연산이 끝난 이후에 사용될 것이다.\n",
    "\n",
    "call 함수는 x_old와 x_new, 두 가지 변수를 필요로 한다. 각각은 입력 값과 이전 층에서의 결과 값을 의미한다. 아래의 코드를 살펴보고, Residual Connection과 Layer Normalization이 코드로 구현된 것을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6859d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResidualConnectionAndLayerNorm:\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"\n",
    "        매개변수(Parameters)\n",
    "        ----------\n",
    "        dim : int\n",
    "        __call__ 함수 설명문 속 d에 해당되는 값이다.\n",
    "        \"\"\"\n",
    "        self.g = mg.ones(dim)\n",
    "        self.b = mg.zeros(dim)\n",
    "\n",
    "\n",
    "    def __call__(self, x_old, x_new):\n",
    "        \"\"\"\n",
    "        매개변수(Parameters)\n",
    "        ----------\n",
    "        x_old : Union[numpy.ndarray, mygrad.Tensor], shape-(..., d)\n",
    "        x_new : mygrad.Tensor, shape-(..., d)\n",
    "\n",
    "        반환 값(Returns)\n",
    "        -------\n",
    "        mygrad.Tensor, shape-(..., d)\n",
    "        \"\"\"\n",
    "        x = x_old + x_new\n",
    "        # 마지막 축을 따라서 정규화를 해주고, 정규화하는 정도는 학습 가능하다.\n",
    "        # 이어서 편향치를 더해주자.\n",
    "        mu = x.mean(axis=-1, keepdims=True)\n",
    "        sigma = x.std(axis=-1, keepdims=True)\n",
    "        return self.g * (x - mu) / (sigma + 1e-6) + self.b\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" 모델의 모든 매개변수를 출력해주는 편리한 함수 \"\"\"\n",
    "        return (self.g, self.b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66915436",
   "metadata": {},
   "source": [
    "이제 트랜스포머의 인코더의 구조를 본격적으로 구현해보자. 트랜스포머의 인코더는 multihead self-attention 층과 feed forward 층, 두 개의 층을 포함할 것이다.\n",
    "\n",
    "feed forward 층의 경우, self-attention 층에서 산출된 문맥 벡터의 유의미한 정보를 추출해주는, 일종의 해석자 역할을 수행한다. 각 층을 거치고 나오게 되면, 그 값에 Residual Connection과 Layer Normalization을 적용해준다. 고로 인코더는 아래와 같은 구조를 보일 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ef2f7",
   "metadata": {},
   "source": [
    "<center>$X_1=MultiHeadAttention(src,src,src)$ Self-Attention Layer\n",
    "<center>$X_2=LayerNorm(src+X_1)$ Residual Connection and Layer Normalization\n",
    "<center>$X_3=ReLU(X_2W_1+\\overrightarrow{b_1})W_2+\\overrightarrow{b_2}$ FeedForward Layer\n",
    "<center>$Enc_{out}=LayerNorm(X_2+X_3)$ Residual Connection and Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f0418",
   "metadata": {},
   "source": [
    "위 식을 참고하여 아래의 두 클래스들을 구현해보자. FeedForward 클래스는 ReLU 활성화 함수를 이용한 간단한 밀집층을 나타낸다. MyNN 라이브러리의 dense 함수를 이용하여 FeedForward 클래스를 구현하고, 위 식을 참고하여 TransformerEncoder 클래스도 구현해보도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10a07eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedForward:\n",
    "    def __init__(self, in_dim, h_dim, out_dim):\n",
    "        \"\"\" 순전파를 위한 간단한 층을 구현해보자.\n",
    "        매개변수(Parameters)\n",
    "        ----------\n",
    "        in_dim : int\n",
    "        입력받는 벡터의 차원\n",
    "        h_dim : int\n",
    "        은닉 층의 뉴런 개수\n",
    "        out_dim : int\n",
    "        출력되는 결과 벡터의 차원\n",
    "\n",
    "        \"\"\"\n",
    "        # MyNN의 밀집 층을 선언해보자.\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        pass\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        매개변수(Parameters)\n",
    "        ----------\n",
    "        x : Union[numpy.ndarray, mygrad.Tensor]\n",
    "        반환 값(Returns)\n",
    "        -------\n",
    "        mygrad.Tensor\n",
    "        \"\"\"\n",
    "        # 두 밀집 층 사이에 ReLU 활성화함수를 적용하자.\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        pass\n",
    "\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" 모델의 모든 매개변수를 출력해주는 편리한 함수 \"\"\"\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        pass\n",
    "\n",
    "class TransformerEncoder:\n",
    "    def __init__(self, dim, h_dim, n_head=3):\n",
    "        \"\"\" 트랜스포머의 인코더를 구현해보자.\n",
    "        매개변수(Parameters)\n",
    "        ----------\n",
    "        dim : int\n",
    "        입력 벡터와 결과 벡터의 차원 ‘d'\n",
    "        잔차 연결을 이용하기 위해 두 벡터의 차원이 같다\n",
    "        h_dim : int\n",
    "        feed forward layer에서의 은닉 창의 뉴런 개수\n",
    "        n_head : int\n",
    "        self-attention를 기반으로 한 층의 attention head 개수\n",
    "        \"\"\"\n",
    "        # 여기에 코드 작성\n",
    "\n",
    "        pass\n",
    "        \n",
    "    def __call__(self, src):\n",
    "        \"\"\" 트랜스포머의 인코더의 순전파를 구현해보자.\n",
    "        매개변수(Parameters)\n",
    "        ----------\n",
    "        src : Union[numpy.ndarray, mygrad.Tensor], shape=(T, N, d)\n",
    "        수열들로 구성된 batch\n",
    "        반환 값(Returns)\n",
    "        -------\n",
    "        mygrad.Tensor, shape=(t, N, d)\n",
    "        \"\"\"\n",
    "        # 입력받은 수열에 self-attention 층을 적용해보자.\n",
    "        # 즉, 입력받은 수열로 attention 층의 query, key, value 벡터를 구성해보자.\n",
    "        # 이를 통해 모델은 입력받은 수열 내의 관계들을 학습한다.\n",
    "        # 여기에 코드 작성\n",
    "\n",
    "        # 첫 residual connection와 layernorm을 이용해보자.\n",
    "        # 여기에 코드 작성\n",
    "\n",
    "        # feedforward 층을 적용하자.\n",
    "        # 여기에 코드 작성\n",
    "\n",
    "        # 두 번째 residual connection와 layernorm을 적용해보자.\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" 모델의 모든 매개변수를 출력해주는 편리한 함수 \"\"\"\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ab880",
   "metadata": {},
   "source": [
    "<h2>6. 디코더</h2>\n",
    "\n",
    "트랜스포머의 디코더는 인코더보다 조금 더 복잡하다. 그 이유는 타겟 시퀀스(정답)가 입력되어야 하기 때문이다. 상식적으로 봤을 때, 답을 예측하기 위한 학습을 진행하는데 입력으로 이미 답을 넣어버리면 그냥 답을 외워버리지 않겠는가? 그럼에도 불구하고 타겟 시퀀스는 왜 입력되는 것일까? 그 이유에 대해 살펴보자.\n",
    "\n",
    "우선, 위와 같은 특이한 상황을 이해하기 위해서는 Teacher Forcing 학습 방법에 대해 알아야 한다. Teacher Forcing이란 무엇인가?\n",
    "\n",
    "누차 들었겠지만, 언어에서는 ‘순서’라는 요소가 굉장히 중요하다. 그렇기에 NLP에서 RNN이 지배적으로 사용되었던 것인데, RNN을 보면 현재 예측값을 가지고 다음 토큰을 예측하는 구조를 보인다. 이런 구조라면 만일 첫 번째 토큰 예측이 잘못 되었다고 했을 때, 이를 가지고 예측하게 될 이후의 토큰들은 당연히 정답과는 거리가 멀 것이다.\n",
    "\n",
    "특정 토큰에 대한 예측이 올바르지 않다면, 그 이후의 모든 토큰들에 대한 예측을 정상적으로 수행할 수 없는 것이다. 과연 바람직한가? 이를 방지하기 위해 Teacher Forcing을 도입할 수 있다. 책에 수록된 그림 3.6-8을 통해 자세히 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a59f34",
   "metadata": {},
   "source": [
    "Teacher Forcing이란, 위 사진처럼 잘못된 예측이 있을 경우에 다음 예측에 사용될 입력 값으로 올바른 예측 값을 집어넣는 것이다. Two 다음에 birds라고 잘못 예측해서 다음에 올 단어로 flying이 출력된 반면에, Teacher Forcing을 통해 birds를 people로 고쳐주면 running이 출력되는 것이다.\n",
    "\n",
    "본 트랜스포머에서도 훈련 과정에서 위와 같은 Teacher Forcing 기법을 활용하기 위해 타겟 시퀀스를 입력받을 것이다. 타겟 시퀀스를 입력해주는 과정에서 만일 타겟 시퀀스의 모든 원소들을 그 값 그대로 입력시켜주게 된다면, 신경망은 정말 타겟 시퀀스를 그대로 외우게 될 것이다. 이를 방지하기 위해서는 입력시켜주는 값에 제한을 걸어두어야 할 것이다. t번째 토큰을 예측하고자 한다면, t+1번째 이후 토큰들에 대한 정보는 알면 안 될 것이다. 이를 위해 self-attention 층을 거친 타겟 시퀀스에 마스크를 씌워줄 것이다. \n",
    "\n",
    "예를 들어, 타겟 시퀀스에 self-attention을 적용한 결과가 아래와 같다고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd0eeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ij = np.array([[0.1, 0.7, 0.2], \n",
    "\t\t[0.4, 0.2, 0.4], \n",
    "\t\t[0.1, 0.8, 0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43d67d",
   "metadata": {},
   "source": [
    "위 배열을 보면, 첫 토큰을 예측할 때 두 번째 토큰이 상당 부분 관여한다는 것을 알 수가 있다. 그러나, 이러한 영향은 바람직하지 않다. 첫 토큰의 경우, 첫 토큰만이 영향을 줄 수 있고, 두 번째 토큰의 경우, 첫 토큰과 두 번 째 토큰의 영향을 받아야 할 것이다. 즉 현재 토큰과 그 이전 토큰들을 가지고 예측해야 하는 것이다. 그렇기에 우리는 현재 토큰 뒤에 있는 토큰들이 현재 토큰을 예측하는데 미치는 영향을 제거할 것이다.\n",
    "\n",
    "**Note** : 어떻게 영향을 제거해줄 수 있을까? 우리가 마스킹하고자 하는 부분의 값을 −∞로 만들어주면 된다. −∞는 이후 softmax 함수에 넣어주었을 때, 그 값이 0이 되기 때문에 해당하는 value 벡터가 아무런 영향을 미치지 못하게 되기 때문이다.\n",
    "\n",
    "softmax를 취하기 전에, a_ij의 모든 원소에 -log를 취해줄 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09bf07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([[-2.30258509, -0.35667494, -1.60943791],\n",
    "              [-0.91629073, -1.60943791, -0.91629073],\n",
    "\t      [-2.30258509, -0.22314355, -2.30258509]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2816923",
   "metadata": {},
   "source": [
    "여기서 우측 상단의 값들을 매우 작은 음수 -1e14로 설정해줌으로써 마스크를 씌워줄 것이다. 이로 인해 softmax를 적용했을 때, 해당하는 값들은 0의 값을 갖게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b79cd659",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([[-2.30258509e+00, -1.00000000e+14, -1.00000000e+14],\n",
    "              [-9.16290730e-01, -1.60943791e+00, -1.00000000e+14],\n",
    "              [-2.30258509e+00, -2.23143550e-01, -2.30258509e+00]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17fe9a",
   "metadata": {},
   "source": [
    "이제 각 행에 대해 softmax를 취해볼 것이다. 매우 작은 음수의 경우 0이 되고, 그렇지 않은 부분들의 경우 합쳤을 때 1이 되어야 할 것이다. softmax를 취해보면 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bde1012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        ],\n",
       "       [0.66666667, 0.33333333, 0.        ],\n",
       "       [0.1       , 0.8       , 0.1       ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1.        , 0.        , 0.        ],\n",
    "       [0.66666667, 0.33333333, 0.        ],\n",
    "       [0.1       , 0.8       , 0.1       ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4f649",
   "metadata": {},
   "source": [
    "이러한 마스킹 매커니즘은 이미 위의 MultiheadAttention 클래스에 구현해놓았다. boolean 타입으로 구성된 배열이며, False에 대응되는 값은 -1e14로 바꾸어준다. 예를 들자면, 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d87a76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array([[ True, False,  True],\n",
    "                 [False,  True, False],\n",
    "                 [ True, False, False]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6dc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([[[[10,  2, -3],\n",
    "                [ 5, -6,  4],\n",
    "                [ 1, -5, 12]]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d649c2",
   "metadata": {},
   "source": [
    "위의 C에 mask를 통해 마스킹을 적용하면, 아래와 같을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e67da19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[              10, -100000000000000,               -3],\n",
       "         [-100000000000000,               -6, -100000000000000],\n",
       "         [               1, -100000000000000, -100000000000000]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[[[ 10,      -100000000000000,          -3],\n",
    "         [-100000000000000,     -6,   -100000000000000],\n",
    "         [       1,   -100000000000000,   -100000000000000]]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9087b041",
   "metadata": {},
   "source": [
    "이런 방식으로 마스킹이 이루어진다. 이제 디코더의 구조를 한 번 살펴보고, 코드로 디코더를 구현해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27832e08",
   "metadata": {},
   "source": [
    "<center>$X_1=MaskedMultiHeadAttention(tgt,tgt,tgt)$ Masked Self-Attention Layer\n",
    "<center>$X_2=LayerNorm(tgt+X_1)$ Residual Connection and Layer Normalization\n",
    "<center>$X_3=MultiHeadAttention(X_2,Enc_{out},Enc_{out})$ Encoder-Decoder Attention Layer\n",
    "<center>$X_4=LayerNorm(X_2+X_3)$ Residual Connection and Layer Normalization\n",
    "<center>$X_5=ReLU(X_4W_1+\\overrightarrow{b_1})W_2+\\overrightarrow{b_2}$ FeedForward Layer\n",
    "<center>$Dec_{out}=LayerNorm(X_4+X_5)$ Residual Connection and Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71186b5f",
   "metadata": {},
   "source": [
    "\n",
    "마스킹을 제외하면, 디코더는 인코더와 닮은 면이 많다. 첫 층에서는 타겟 시퀀스를 입력받아서 self-attention 기법을 적용한다. 그리고 Add & Norm을 거친 다음, 인코더의 결과와 첫 층의 결과 값을 입력받아서 attention을 적용시킨다. 마찬가지로 Add & Norm을 거친 후, feed forward 층을 통과시켜서 attention 적용된 값을 해석해 준다.\n",
    "\n",
    "자 이제, 아래의 TransformerDecoder 클래스를 채워보자. 타겟 시퀀스를 받는 self-attention 층에만 마스크를 적용하고, 나머지 층에는 마스크가 쓰이지 않는다는 점에 유의하도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f30903d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerDecoder:\n",
    "    def __init__(self, dim, h_dim, n_head=3):\n",
    "        \"\"\" TransformerDecoder를 구현해보자.\n",
    "        \n",
    "        매개변수(Parameters)\n",
    "        ----------\n",
    "        dim : int\n",
    "            입력받는, 출력할 값의 임베딩 차원의 수 (‘d’)\n",
    "        \n",
    "        h_dim : int\n",
    "            feed forward 층의 hidden 층의 뉴런의 개수\n",
    "        \n",
    "        n_head : int\n",
    "            multihead attention 층에서의 attention head 개수\n",
    "        \"\"\"\n",
    "        # 두 개의 attention 층을 구축하자\n",
    "        # 그 중 하나는 타겟 시퀀스를 가지고 self-attention을 적용한다.\n",
    "        # 다른 하나는 타겟 시퀀스와 인코더의 출력 값을 가지고 attention을 계산한다.\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        # feed forward 층과 3개의 residual connection + layer norm 층을 구축해보자.\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def __call__(self, enc_out, tgt):\n",
    "        \"\"\"\n",
    "        \n",
    "        매개변수(Parameters)\n",
    "        ----------\n",
    "        enc_out : mygrad.Tensor, shape=(T, N, d)\n",
    "            인코더의 출력값\n",
    "        \n",
    "        tgt : Union[numpy.array, mygrad.Tensor], shape=(t, N, d)\n",
    "            번역이 이루어진 타겟 시퀀스들로 구성된 batch\n",
    "\n",
    "        반환 값(Returns)\n",
    "        -------\n",
    "        mygrad.Tensor, shape=(T, N, d)\n",
    "       \n",
    "        \"\"\"\n",
    "        # 타겟 시퀀스에 self-attention 적용하는 층에 활용할 마스크를 만들어보자.\n",
    "        # attention score의 우측 상단 부분을 -1e14로 만들 것이다.\n",
    "        # 마스크는 반드시 (t, t) 모양의 Numpy boolean 배열이어야 한다.\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        # 타겟 시퀀스에 마스킹이 포함된 self-attention 층을 적용해보자.\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        # 첫 번째 layer norm을 적용해보자.\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        # 인코더-디코더 attention 층을 구축해보자.\n",
    "        # query 값으로 타겟 시퀀스에 self-attention 적용된 값을 넣는다.\n",
    "        # key와 value 값으로 인코더의 출력 값을 넣는다.\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        # 두 번째 layer norm을 적용해보자.\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        # feed forward 층을 구축해보자.\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        # layer norm을 마지막으로 적용해보자.\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" 모델의 모든 매개변수들을 출력해주는 편리한 함수 \"\"\"\n",
    "        # 여기에 코드 작성\n",
    "        \n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
