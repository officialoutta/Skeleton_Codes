{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-BSIoRVMwcr"
   },
   "source": [
    "# **실습 2. 간단한 말뭉치 분석으로 영어 텍스트의 특징 파악하기**\n",
    "본 실습에서 사용하는 데이터셋(wikipedia2text-extracted.txt)의 출처는 다음과 같음을 밝힌다.\n",
    "\n",
    "https://www.evanjones.ca/software/wikipedia2text.html\n",
    "\n",
    "## **0. 실습 세팅 하기**\n",
    "이번 실습에서는 파이썬을 이용하여 글자와 단어 단위 각각에서 영어 텍스트를 분석하고, 그 결과를 기반으로 영어 텍스트의 특징을 추론해볼 것이다. 본격적인 과제 이행에 앞서 과제에 필요한 세팅을 진행해보자.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "잠깐! 본격적인 과제 시작 전, Python 문자열에 대해 간략히 복습해보자. \n",
    "\n",
    "우선, 문자열은 각 문자의 순서 정보가 중요한 ’시퀀스‘ 객체임을 기억하며 시퀀스의 성질 4가지를 기억하자. 첫째, 튜플처럼 고정되어 있으며, 둘째, 인덱싱/슬라이싱 기능이 지원되고, 셋째, 문자별로 반복(iterate)도 가능하다. 마지막으로, 문자열의 길이는 문자열을 이루는 문자 개수로 결정된다. (줄 바꿈 \\n과 같은 특수 문자 역시 단일 문자이다.)\n",
    "\n",
    "다음은 문자열 작업에 유용한 몇 가지 리소스이다. 본 활동에서는 다양한 파이썬 문자열 메서드를 사용하니, 별도의 윈도우 창에 열어두고 사용하는 것을 추천한다.\n",
    "- Python 문자열에 대한 기초 튜토리얼 \n",
    ": https://docs.python.org/3/tutorial/introduction.html#strings\n",
    "- 문자열 메서드 리스트\n",
    ": https://docs.python.org/3/library/stdtypes.html#string-methods\n",
    "\n",
    "\n",
    "---\n",
    "활동에 필요한 모듈인 time과 numpy, matplotlib 등을 호출해보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72p5hc0HSZ3r"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKSAaxkeSdjX"
   },
   "source": [
    "\n",
    "우리는 'unzip' 함수를 사용할 것인데, unzip은 여러 아이템 모음을 별개의 튜플로 \"풀어주는(unzip)\" 함수이다. 예컨대, pairs = [(\"a\", 1), (\"b\", 2), ...]에 unzip을 실행하면 ((\"a\", \"b\", ...), (1, 2, ...))와 같이 여러 pair들이 풀어져 새로운 튜플을 구성하게 된다. 해당 함수를 각자 컴퓨터에서 다양하게 실행해보며 함수에 대한 직관적 이해를 높여보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMpyEiRCSfWS"
   },
   "outputs": [],
   "source": [
    "def unzip(pairs):\n",
    "    \"\"\"    \n",
    "    매개변수\n",
    "    ----------\n",
    "    pairs : Iterable[Tuple[Any, ...]]\n",
    "        (a0, b0, c0, ...), (a1, b1, c1, ...)) 형태의 이터러블\n",
    "    \n",
    "    반환 값\n",
    "    -------\n",
    "    Tuple[Tuples[Any, ...], ...]\n",
    "       ‘여러 쌍들(pairs)’이 ‘풀어진(unzipped)’ 내용물로 구성된 하나의 튜플; i.e. \n",
    "       ((a0, a1, ...), (b0, b1, ...), (c0, c1), ...)\n",
    "    \"\"\"\n",
    "    return tuple(zip(*pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCVfyr0lTH03"
   },
   "source": [
    "## **1.  영어에서 가장 자주 사용되는 글자는 무엇일까?**\n",
    "게임 Wheel of Foretune의 보너스 라운드에서 참가자들은 3개의 자음과 1개의 모음을 추가로 선택하게 된다. 선택 전 기본적으로 제공되는 알파벳에는 항상 R, S, T, L, N, E가 있다. 여러 알파벳 가운데 왜 하필 R, S, T, L, N, E가 힌트로 제공되는 것일까? 위키피디아 내 영어 텍스트의 특정 말뭉치를 분석해보면, 자연스럽게 질문에 대한 답을 찾을 수 있게 된다. 지금부터 영어 텍스트 말뭉치를 분석하여, 알파벳 사용의 특징을 알아내보자!\n",
    "\n",
    "### **1.0 데이터 준비하기**  \n",
    "각자의 컴퓨터에 Evan Jones의 \"wikipedia2text-extracted.txt\" 전체 내용을 단일 문자열로 로드하라. 이 문서 중 일부는 ASCII가 아닌 것(ex. 몇몇 중국어 글자)을 포함하기에, 파일을 이진 읽기 모드 <mode='rb'>로 열어야 한다. 이를 통해, 일반적인 문자열 형태가 아닌, 문자에 대한 컴퓨터의 메모리 인코딩인 <bytes>형태로 읽을 수 있게 된다. 만약 <bytes> 형태를 친숙한 문자열의 형태로 디코딩하려면, 이 bytes 인스턴스에서 메서드 <decode>를 호출하면 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7Up-kvrTyV2"
   },
   "outputs": [],
   "source": [
    "with open(path_to_wikipedia, \"rb\") as f:\n",
    "    # bytes를 string으로 디코딩\n",
    "    wikipedia = f.read().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouCP2EcYTzGD"
   },
   "source": [
    "더불어 중요한 것은, 바이트에서 문자열로 디코딩한 후에는 문자열의 모든 문자를 소문자로 만들어야 하는데, 이때 <lower> 메서드를 활용할 수 있다. \n",
    "\n",
    "위 조건과 힌트를 모두 반영하여 데이터를 출력하는 코드를 작성해보자. 이를 통해 문자열에 총 6,300만 자 이상이 있다는 것을 직접 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXQ2o_O8T7gW"
   },
   "outputs": [],
   "source": [
    "path_to_wikipedia = \"파일 경로\"\n",
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_s1mnKYMT71k"
   },
   "source": [
    "인덱스 기능을 활용하여, 문자열 가운데 앞부분에 위치한 500개 글자를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdkODL5tUAbn"
   },
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGQ9HvwnUBxR"
   },
   "source": [
    "### **1.1 텍스트의 글자 수 세기**\n",
    "데이터가 준비되었음을 확인하였으니, 본격적으로 wikipedia 말뭉치를 이루는 모든 letter의 개수를 빠짐 없이 세어 보자.  \n",
    "단어 character와 letter의 경우 별도의 번역 없이 영어 그대로 표현하겠다. 한국어로는 character과 letter을 모두 ‘문자’ 혹은 ‘글자’라고 번역하는데, 영어에서는 character를 letter의 상위 개념으로 보며 둘을 엄밀하게 구분한다. (character는 알파벳과 같은 문자뿐 아니라 숫자나 부호까지 포함한다.) 이 지점에서 한국어 번역에 어려움이 생겼기에 영어 원 단어를 있는 그대로 사용하기로 하였다.  \n",
    "먼저, 구두점 및 < \\n>과 같은 특수 문자를 포함하여 모든 character의 발생 횟수를 계산해보자.\n",
    "* 힌트: 여러분은 파일의 모든 character 개수(count)를 한 줄로 생성할 줄 알아야 한다. Python의 collection 모듈에 있는 Counter 클래스는 각 원소별 개수를 딕셔너리 형태로 저장해주는 기능을 수행한다. Counter 클래스를 활용하면 이를 활용하면 약 5초 만에 작업을 완료할 수 있으며, 같은 일에 for 반복문을 사용한다면 약 10배 더 오래 걸릴 것이다. Counter 클래스는 추후 실습에서도 자주 등장하니 꼭 기억해두도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKRhUVGaUObo"
   },
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpT0-UMUUPOk"
   },
   "source": [
    "이제 우리는 내림차순으로 정렬된 letter-count 튜플 목록을 만들어야 한다. 이때 ① letter-count에는 불필요한 특수문자가 제외된 순수한 letter만 있어야 하며, ② count 값을 기준으로 내림차순으로 정렬해야 한다. 여러분이 두 가지 조건을 충족시키는 코드를 작성할 수 있도록 힌트를 드리겠다.\n",
    "\n",
    "\n",
    "---\n",
    "① 현재 소문자화가 이루어진 상태이므로, 특수문자를 제외한 letter만 취하기 위해서 string 모듈의 string.ascii_lowercase을 활용할 수 있다.  \n",
    "```\n",
    "import string  \n",
    "string.ascii_lowercase  \n",
    "'abcdefghijklmnopqrstuvwxyz'  \n",
    "```\n",
    "② Counter 클래스에 있는 <most_common>메서드를 사용하면, 우리의 character-count 튜플을 내림차순으로 정렬해줄 수 있다. \n",
    "\n",
    "\n",
    "\n",
    ">두 가지 조건을 반영한 결과물은 총 길이 26의 list 형태여야 한다.\n",
    "\n",
    ">[('e', 6091134),  \n",
    " ('t', 4456786),  \n",
    " ('a', 4365050),  \n",
    " ('i', 3866921),  \n",
    " ('n', 3740382),  \n",
    " ('o', 3676394),  \n",
    " ...  \n",
    "\n",
    "직접 코드를 작성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ct_ZyeSOU-xd"
   },
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZilRs8jhU7BC"
   },
   "source": [
    "이번에는 <freqs>라는 변수를 만들어보자. 변수 <freqs>는 튜플로 구성된 리스트이며, 이번에는 letter-count 쌍이 아닌 letter-frequency(빈도) 쌍을 취하는 것이다. 이때 ‘빈도’는 특정  letter의 개수를 말뭉치 내 전체 letter의 개수로 나눈 값으로, 일종의 비율(ratio)이다. letter-frequency는 아래와 같은 형태로 나타나게 된다.  \n",
    ">[('e', 0.12081350306248849),  \n",
    " ('t', 0.088397321263964282),  \n",
    " ('a', 0.0865778000521603),  \n",
    " ...  \n",
    " ('q', 0.0010429083984244488)]  \n",
    "\n",
    " 주의! 전체 letter의 개수를 계산할 때 앞서 계산한 letter-count 튜플을 그대로 합해서는 안 되며, 튜플 내 숫자 부분끼리의 합을 계산해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7XhwloQVQOK"
   },
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Amm7rt7wVRxZ"
   },
   "source": [
    "각 letter에 대한 빈도(frequency) 값이 모두 합해졌을 때 ‘1’이 나오는지 확인해보아라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-eNRvwhVSN-"
   },
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wXsN3GuVTJ8"
   },
   "source": [
    "### **1.2 letter frequency 히스토그램 그리기**  \n",
    "지금부터는 letter의 빈도를 히스토그램으로 시각화해보도록 하겠다. 여러분이 직접 코드를 작성해볼텐데, 이를 위한 가이드라인을 제시해주겠다.  \n",
    "\n",
    "\n",
    "---\n",
    "① 기본 세팅 코드\n",
    "fig, ax = plt.subplots()\n",
    "labels, values = unzip(freqs)\n",
    "\n",
    "② <ax.bar>를 활용하여(주피터 노트북의 Shift 탭 사용할 것) letter와 각 letter에 대한 빈도를 나타내는 막대 그래프를 그려보아라. 이때 x축은 letter-frequency에 대한 내림차순으로 각 letter를 정렬해야 한다.\n",
    "\n",
    "③ 눈금 표시도 해야 한다. 우리는 그래프의 축에 간격을 구분하기 위해 표시하는 눈금을  ‘틱(Tick)’이라고 부른다. 우선, <ax.set_xticks>를 사용하여 그래프에 사용할 x-tick 값의 범위를 지정해야 하는데, labels의 길이로 범위를 정하는 것이 좋을 것이다. 이후,  <ax.set_xticklabels>를 사용하여 x축을 따라 눈금에 대한 labels(예: 'e', 't', 'a'...)을 나타내어라.\n",
    "\n",
    "④ y축의 제목과 레이블 역시 지정해야 한다. 이를 위해 <set_ylabel>과 <set_title>을 사용할 수 있다.\n",
    "\n",
    "\n",
    "---\n",
    "위의 네 가지 조건과 힌트를 반영하여 직접 코드를 작성해보자.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySb-HGnnVaGv"
   },
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jr7KbEyBVbo-"
   },
   "source": [
    "## **2. 영어에서 가장 자주 사용되는 단어는 무엇일까?**\n",
    "\n",
    "지금까지는 글자 수준에서 영어 사용 분포를 파악해보았고, 이제부터는 한 수준 올라가  단어 수준에서의 분포를 살펴볼 것이다. 이를 위해 우리가 이전에 소문자로 바꿔두었던 위키백과 말뭉치로 다시 돌아가야 한다. 이번 활동에서의 핵심은 토큰화 작업이다.\n",
    "\n",
    "### **2.0 간단한 토큰화(Tokenization)**\n",
    "우선, 문자열을 여백 글자(ex. 공백, 탭, 줄 바꿈 등)을 기준으로 분할하는 간단한 토큰화 방식을 실행해보자. 말뭉치를 분할하여 ’tokens' list를 만든 후, 토큰 총개수와 처음 10개의 토큰을 출력하는 코드를 작성해보자. 이때 for-반복문을 사용하지 말고 split 함수를 활용하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qa_33g0oVf-f"
   },
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lt8A2PCUVh-X"
   },
   "source": [
    "### **2.1 텍스트의 단어 수 세기**\n",
    "1.1번 활동과 동일한 방식(collections 모듈의 Counter를 활용)을 활용하여, 이번에는 단어(words)의 개수를 출력해보아라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCIh0wEVVkBT"
   },
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4IAmgXUVk4G"
   },
   "source": [
    "마지막으로, 인덱스를 활용하여 가장 자주 발생한 상위 20개 단어를 출력하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pQPH6MhVmG9"
   },
   "outputs": [],
   "source": [
    "# 여기에 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvURQnB5Vm2L"
   },
   "source": [
    "출력 결과를 살펴보자. 상위 20개 단어 list의 특징은 무엇인가? 이 list의 최상단에 사람이나 장소에 대한 고유명사나 독특한 단어를 찾을 수 있는가? 당신은 상위 20개의 단어들로부터 글의 내용에 대한 중요한 정보를 얻어낼 수 있는가? 아마 그렇지 않을 것이다. 이와 같이 자주 등장하지만 핵심 의미를 지니지 않은 채 문법적 기능을 수행하는 단어를 자연어 처리에서 ‘불용어’라고 일컫는다.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_실습2",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
